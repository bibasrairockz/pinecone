{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd7ee9b-705b-4928-acbf-383122980b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (0.1.13)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (0.0.29)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (0.1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-3.2.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pinecone-client) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pinecone-client) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pinecone-client) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-3.2.1-py3-none-any.whl (213 kB)\n",
      "   ---------------------------------------- 0.0/214.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 214.0/214.0 kB 12.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.2.1\n",
      "Requirement already satisfied: pypdf in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pypdf) (4.10.0)\n",
      "Requirement already satisfied: openai in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (1.14.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (1.10.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install pypdf\n",
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da779c29-4c19-4375-9ebd-5245d665ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05773102-d6c3-44cf-9d2e-16f87ac57160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file pdfs already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7be23a08-e23b-4543-973b-e3956508e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "83b106e9-77b6-4264-a698-b39583e73144",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9af4800a-7e18-472d-a2f6-b8ddce533e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='arXiv:1409.1556v6  [cs.CV]  10 Apr 2015Publishedasa conferencepaperat ICLR2015\\nVERYDEEPCONVOLUTIONAL NETWORKS\\nFORLARGE-SCALEIMAGERECOGNITION\\nKarenSimonyan∗& AndrewZisserman+\\nVisualGeometryGroup,DepartmentofEngineeringScience, UniversityofOxford\\n{karen,az }@robots.ox.ac.uk\\nABSTRACT\\nIn this work we investigate the effect of the convolutional n etwork depth on its\\naccuracy in the large-scale image recognition setting. Our main contribution is\\na thorough evaluation of networks of increasing depth using an architecture with\\nverysmall( 3×3)convolutionﬁlters,whichshowsthatasigniﬁcantimprove ment\\non the prior-art conﬁgurations can be achieved by pushing th e depth to 16–19\\nweight layers. These ﬁndings were the basis of our ImageNet C hallenge 2014\\nsubmission,whereourteamsecuredtheﬁrstandthesecondpl acesinthelocalisa-\\ntion and classiﬁcation tracks respectively. We also show th at our representations\\ngeneralise well to other datasets, where they achieve state -of-the-art results. We\\nhave made our two best-performingConvNet models publicly a vailable to facili-\\ntate furtherresearchontheuse ofdeepvisualrepresentati onsincomputervision.\\n1 INTRODUCTION\\nConvolutional networks (ConvNets) have recently enjoyed a great success in large-scale im-\\nage and video recognition (Krizhevskyetal., 2012; Zeiler& Fergus, 2013; Sermanetet al., 2014;\\nSimonyan& Zisserman, 2014) which has become possible due to the large public image reposito-\\nries,suchasImageNet(Denget al.,2009),andhigh-perform ancecomputingsystems,suchasGPUs\\norlarge-scaledistributedclusters(Deanet al., 2012). In particular,animportantroleintheadvance\\nofdeepvisualrecognitionarchitectureshasbeenplayedby theImageNetLarge-ScaleVisualRecog-\\nnition Challenge (ILSVRC) (Russakovskyet al., 2014), whic h has served as a testbed for a few\\ngenerationsof large-scale image classiﬁcation systems, f rom high-dimensionalshallow feature en-\\ncodings(Perronninetal.,2010)(thewinnerofILSVRC-2011 )todeepConvNets(Krizhevskyet al.,\\n2012)(thewinnerofILSVRC-2012).\\nWith ConvNets becoming more of a commodity in the computer vi sion ﬁeld, a number of at-\\ntempts have been made to improve the original architecture o f Krizhevskyet al. (2012) in a\\nbid to achieve better accuracy. For instance, the best-perf orming submissions to the ILSVRC-\\n2013 (Zeiler&Fergus, 2013; Sermanetetal., 2014) utilised smaller receptive window size and\\nsmaller stride of the ﬁrst convolutional layer. Another lin e of improvements dealt with training\\nand testing the networks densely over the whole image and ove r multiple scales (Sermanetet al.,\\n2014; Howard, 2014). In this paper, we address another impor tant aspect of ConvNet architecture\\ndesign–itsdepth. Tothisend,we ﬁxotherparametersofthea rchitecture,andsteadilyincreasethe\\ndepth of the network by adding more convolutionallayers, wh ich is feasible due to the use of very\\nsmall (3×3)convolutionﬁltersinall layers.\\nAs a result, we come up with signiﬁcantly more accurate ConvN et architectures, which not only\\nachieve the state-of-the-art accuracy on ILSVRC classiﬁca tion and localisation tasks, but are also\\napplicabletootherimagerecognitiondatasets,wherethey achieveexcellentperformanceevenwhen\\nusedasa partofa relativelysimple pipelines(e.g.deepfea turesclassiﬁed byalinearSVM without\\nﬁne-tuning). We havereleasedourtwobest-performingmode ls1tofacilitatefurtherresearch.\\nThe rest of the paper is organised as follows. In Sect. 2, we de scribe our ConvNet conﬁgurations.\\nThe details of the image classiﬁcation trainingand evaluat ionare then presented in Sect. 3, and the\\n∗current afﬁliation: Google DeepMind+current afﬁliation: Universityof Oxfordand Google DeepMi nd\\n1http://www.robots.ox.ac.uk/ ˜vgg/research/very_deep/\\n1', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df553196-bb42-46bd-93d4-a8df722cef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c6faf7f-7fbd-42ab-b3e4-abab0cd3042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2869100e-ee03-44fe-96f7-7bc824742238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='arXiv:1409.1556v6  [cs.CV]  10 Apr 2015Publishedasa conferencepaperat ICLR2015\\nVERYDEEPCONVOLUTIONAL NETWORKS\\nFORLARGE-SCALEIMAGERECOGNITION\\nKarenSimonyan∗& AndrewZisserman+\\nVisualGeometryGroup,DepartmentofEngineeringScience, UniversityofOxford\\n{karen,az }@robots.ox.ac.uk\\nABSTRACT\\nIn this work we investigate the effect of the convolutional n etwork depth on its\\naccuracy in the large-scale image recognition setting. Our main contribution is', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='a thorough evaluation of networks of increasing depth using an architecture with\\nverysmall( 3×3)convolutionﬁlters,whichshowsthatasigniﬁcantimprove ment\\non the prior-art conﬁgurations can be achieved by pushing th e depth to 16–19\\nweight layers. These ﬁndings were the basis of our ImageNet C hallenge 2014\\nsubmission,whereourteamsecuredtheﬁrstandthesecondpl acesinthelocalisa-\\ntion and classiﬁcation tracks respectively. We also show th at our representations', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='generalise well to other datasets, where they achieve state -of-the-art results. We\\nhave made our two best-performingConvNet models publicly a vailable to facili-\\ntate furtherresearchontheuse ofdeepvisualrepresentati onsincomputervision.\\n1 INTRODUCTION\\nConvolutional networks (ConvNets) have recently enjoyed a great success in large-scale im-\\nage and video recognition (Krizhevskyetal., 2012; Zeiler& Fergus, 2013; Sermanetet al., 2014;', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='Simonyan& Zisserman, 2014) which has become possible due to the large public image reposito-\\nries,suchasImageNet(Denget al.,2009),andhigh-perform ancecomputingsystems,suchasGPUs\\norlarge-scaledistributedclusters(Deanet al., 2012). In particular,animportantroleintheadvance\\nofdeepvisualrecognitionarchitectureshasbeenplayedby theImageNetLarge-ScaleVisualRecog-\\nnition Challenge (ILSVRC) (Russakovskyet al., 2014), whic h has served as a testbed for a few', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='generationsof large-scale image classiﬁcation systems, f rom high-dimensionalshallow feature en-\\ncodings(Perronninetal.,2010)(thewinnerofILSVRC-2011 )todeepConvNets(Krizhevskyet al.,\\n2012)(thewinnerofILSVRC-2012).\\nWith ConvNets becoming more of a commodity in the computer vi sion ﬁeld, a number of at-\\ntempts have been made to improve the original architecture o f Krizhevskyet al. (2012) in a\\nbid to achieve better accuracy. For instance, the best-perf orming submissions to the ILSVRC-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='2013 (Zeiler&Fergus, 2013; Sermanetetal., 2014) utilised smaller receptive window size and\\nsmaller stride of the ﬁrst convolutional layer. Another lin e of improvements dealt with training\\nand testing the networks densely over the whole image and ove r multiple scales (Sermanetet al.,\\n2014; Howard, 2014). In this paper, we address another impor tant aspect of ConvNet architecture\\ndesign–itsdepth. Tothisend,we ﬁxotherparametersofthea rchitecture,andsteadilyincreasethe', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='depth of the network by adding more convolutionallayers, wh ich is feasible due to the use of very\\nsmall (3×3)convolutionﬁltersinall layers.\\nAs a result, we come up with signiﬁcantly more accurate ConvN et architectures, which not only\\nachieve the state-of-the-art accuracy on ILSVRC classiﬁca tion and localisation tasks, but are also\\napplicabletootherimagerecognitiondatasets,wherethey achieveexcellentperformanceevenwhen', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='usedasa partofa relativelysimple pipelines(e.g.deepfea turesclassiﬁed byalinearSVM without\\nﬁne-tuning). We havereleasedourtwobest-performingmode ls1tofacilitatefurtherresearch.\\nThe rest of the paper is organised as follows. In Sect. 2, we de scribe our ConvNet conﬁgurations.\\nThe details of the image classiﬁcation trainingand evaluat ionare then presented in Sect. 3, and the\\n∗current afﬁliation: Google DeepMind+current afﬁliation: Universityof Oxfordand Google DeepMi nd', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='1http://www.robots.ox.ac.uk/ ˜vgg/research/very_deep/\\n1', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 0}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nconﬁgurations are compared on the ILSVRC classiﬁcation tas k in Sect. 4. Sect. 5 concludes the\\npaper. For completeness,we also describeand assess our ILS VRC-2014object localisationsystem\\ninAppendixA,anddiscussthegeneralisationofverydeepfe aturestootherdatasetsinAppendixB.\\nFinally,AppendixCcontainsthelist ofmajorpaperrevisio ns.\\n2 CONVNETCONFIGURATIONS\\nTo measure the improvement brought by the increased ConvNet depth in a fair setting, all our', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='ConvNet layer conﬁgurations are designed using the same pri nciples, inspired by Ciresan etal.\\n(2011); Krizhevskyet al. (2012). In this section, we ﬁrst de scribe a generic layout of our ConvNet\\nconﬁgurations(Sect.2.1)andthendetailthespeciﬁcconﬁg urationsusedintheevaluation(Sect.2.2).\\nOurdesignchoicesarethendiscussedandcomparedtothepri orart inSect. 2.3.\\n2.1 A RCHITECTURE\\nDuring training, the input to our ConvNets is a ﬁxed-size 224×224RGB image. The only pre-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='processingwedoissubtractingthemeanRGBvalue,computed onthetrainingset,fromeachpixel.\\nTheimageispassedthroughastackofconvolutional(conv.) layers,whereweuseﬁlterswithavery\\nsmall receptive ﬁeld: 3×3(which is the smallest size to capture the notion of left/rig ht, up/down,\\ncenter). In one of the conﬁgurationswe also utilise 1×1convolutionﬁlters, which can be seen as\\na linear transformationof the input channels (followed by n on-linearity). The convolutionstride is', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='ﬁxedto1pixel;thespatialpaddingofconv.layerinputissuchthatt hespatialresolutionispreserved\\nafterconvolution,i.e. the paddingis 1pixel for3×3conv.layers. Spatial poolingis carriedoutby\\nﬁvemax-poolinglayers,whichfollowsomeoftheconv.layer s(notalltheconv.layersarefollowed\\nbymax-pooling). Max-poolingisperformedovera 2×2pixelwindow,withstride 2.\\nAstackofconvolutionallayers(whichhasadifferentdepth indifferentarchitectures)isfollowedby', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='three Fully-Connected(FC) layers: the ﬁrst two have4096ch annelseach,the thirdperforms1000-\\nway ILSVRC classiﬁcation and thus contains1000channels(o ne foreach class). The ﬁnal layer is\\nthesoft-maxlayer. Theconﬁgurationofthefullyconnected layersis thesameinall networks.\\nAllhiddenlayersareequippedwiththerectiﬁcation(ReLU( Krizhevskyetal.,2012))non-linearity.\\nWe note that none of our networks (except for one) contain Loc al Response Normalisation', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='(LRN) normalisation (Krizhevskyet al., 2012): as will be sh own in Sect. 4, such normalisation\\ndoes not improve the performance on the ILSVRC dataset, but l eads to increased memory con-\\nsumption and computation time. Where applicable, the param eters for the LRN layer are those\\nof(Krizhevskyetal., 2012).\\n2.2 C ONFIGURATIONS\\nThe ConvNet conﬁgurations, evaluated in this paper, are out lined in Table 1, one per column. In', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='the following we will refer to the nets by their names (A–E). A ll conﬁgurationsfollow the generic\\ndesign presented in Sect. 2.1, and differ only in the depth: f rom 11 weight layers in the network A\\n(8conv.and3FClayers)to19weightlayersinthenetworkE(1 6conv.and3FClayers). Thewidth\\nof conv.layers (the number of channels) is rather small, sta rting from 64in the ﬁrst layer and then\\nincreasingbyafactorof 2aftereachmax-poolinglayer,untilit reaches 512.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='In Table 2 we reportthe numberof parametersfor each conﬁgur ation. In spite of a large depth, the\\nnumberof weights in our netsis not greater thanthe numberof weightsin a moreshallow net with\\nlargerconv.layerwidthsandreceptiveﬁelds(144Mweights in(Sermanetet al., 2014)).\\n2.3 D ISCUSSION\\nOur ConvNet conﬁgurations are quite different from the ones used in the top-performing entries\\nof the ILSVRC-2012 (Krizhevskyetal., 2012) and ILSVRC-201 3 competitions (Zeiler& Fergus,', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='2013;Sermanetet al.,2014). Ratherthanusingrelativelyl argereceptiveﬁeldsintheﬁrstconv.lay-\\ners(e.g.11×11withstride 4in(Krizhevskyet al.,2012),or 7×7withstride 2in(Zeiler& Fergus,\\n2013; Sermanetet al., 2014)), we use very small 3×3receptive ﬁelds throughout the whole net,\\nwhichareconvolvedwiththeinputateverypixel(withstrid e1). Itiseasytoseethatastackoftwo\\n3×3conv.layers(withoutspatialpoolinginbetween)hasaneff ectivereceptiveﬁeldof 5×5;three\\n2', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 1}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nTable 1:ConvNet conﬁgurations (shown in columns). The depth of the conﬁgurations increase s\\nfromtheleft(A)totheright(E),asmorelayersareadded(th eaddedlayersareshowninbold). The\\nconvolutional layer parameters are denoted as “conv ⟨receptive ﬁeld size ⟩-⟨number of channels ⟩”.\\nTheReLU activationfunctionisnotshownforbrevity.\\nConvNet Conﬁguration\\nA A-LRN B C D E\\n11weight 11weight 13 weight 16weight 16weight 19 weight\\nlayers layers layers layers layers layers', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='input (224×224RGBimage)\\nconv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64\\nLRN conv3-64 conv3-64 conv3-64 conv3-64\\nmaxpool\\nconv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128\\nconv3-128 conv3-128 conv3-128 conv3-128\\nmaxpool\\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\\nconv1-256 conv3-256 conv3-256\\nconv3-256\\nmaxpool\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv1-512 conv3-512 conv3-512\\nconv3-512\\nmaxpool\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv1-512 conv3-512 conv3-512\\nconv3-512\\nmaxpool\\nFC-4096\\nFC-4096\\nFC-1000\\nsoft-max\\nTable2:Number ofparameters (inmillions).\\nNetwork A,A-LRN BCDE\\nNumber of parameters 133 133134138144', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='such layers have a 7×7effectivereceptive ﬁeld. So what have we gainedby using, fo r instance, a\\nstackofthree 3×3conv.layersinsteadofasingle 7×7layer? First,weincorporatethreenon-linear\\nrectiﬁcation layers instead of a single one, which makes the decision functionmore discriminative.\\nSecond, we decrease the number of parameters: assuming that both the input and the output of a\\nthree-layer 3×3convolutionstack has Cchannels,the stack is parametrisedby 3(\\n32C2)\\n= 27C2', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='32C2)\\n= 27C2\\nweights; at the same time, a single 7×7conv. layer would require 72C2= 49C2parameters, i.e.\\n81%more. Thiscan be seen as imposinga regularisationon the 7×7conv.ﬁlters, forcingthemto\\nhaveadecompositionthroughthe 3×3ﬁlters(withnon-linearityinjectedin between).\\nThe incorporation of 1×1conv. layers (conﬁguration C, Table 1) is a way to increase th e non-\\nlinearity of the decision function without affecting the re ceptive ﬁelds of the conv. layers. Even', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='thoughinourcasethe 1×1convolutionisessentiallyalinearprojectionontothespa ceofthesame\\ndimensionality(thenumberofinputandoutputchannelsist hesame),anadditionalnon-linearityis\\nintroducedbytherectiﬁcationfunction. Itshouldbenoted that1×1conv.layershaverecentlybeen\\nutilisedin the“NetworkinNetwork”architectureofLinet a l.(2014).\\nSmall-size convolution ﬁlters have been previously used by Ciresan etal. (2011), but their nets', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='are signiﬁcantly less deep than ours, and they did not evalua te on the large-scale ILSVRC\\ndataset. Goodfellowet al. (2014) applied deep ConvNets ( 11weight layers) to the task of\\nstreet number recognition, and showed that the increased de pth led to better performance.\\nGoogLeNet(Szegedyet al., 2014), a top-performingentryof the ILSVRC-2014classiﬁcation task,\\nwas developed independentlyof our work, but is similar in th at it is based on very deep ConvNets\\n3', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 2}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\n(22 weight layers) and small convolution ﬁlters (apart from 3×3, they also use 1×1and5×5\\nconvolutions). Their network topology is, however, more co mplex than ours, and the spatial reso-\\nlution of the feature maps is reduced more aggressively in th e ﬁrst layers to decrease the amount\\nof computation. As will be shown in Sect. 4.5, our model is out performing that of Szegedyetal.\\n(2014)intermsofthesingle-networkclassiﬁcationaccura cy.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='3 CLASSIFICATION FRAMEWORK\\nIn the previous section we presented the details of our netwo rk conﬁgurations. In this section, we\\ndescribethe detailsofclassiﬁcationConvNettrainingand evaluation.\\n3.1 T RAINING\\nThe ConvNet training procedure generally follows Krizhevs kyetal. (2012) (except for sampling\\ntheinputcropsfrommulti-scaletrainingimages,asexplai nedlater). Namely,thetrainingiscarried\\nout by optimising the multinomial logistic regression obje ctive using mini-batch gradient descent', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='(based on back-propagation(LeCunet al., 1989)) with momen tum. The batch size was set to 256,\\nmomentum to 0.9. The training was regularised by weight decay (the L2penalty multiplier set to\\n5·10−4)anddropoutregularisationfortheﬁrsttwofully-connect edlayers(dropoutratiosetto 0.5).\\nThelearningrate wasinitially setto 10−2,andthendecreasedbyafactorof 10whenthevalidation\\nset accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='was stopped after 370K iterations (74 epochs). We conjecture that in spite of the l arger number of\\nparametersandthegreaterdepthofournetscomparedto(Kri zhevskyetal.,2012),thenetsrequired\\nlessepochstoconvergedueto(a)implicitregularisationi mposedbygreaterdepthandsmallerconv.\\nﬁlter sizes; (b)pre-initialisationofcertainlayers.\\nThe initialisation of the networkweightsis important,sin ce bad initialisation can stall learningdue', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='to the instability of gradient in deep nets. To circumvent th is problem, we began with training\\nthe conﬁgurationA (Table 1), shallow enoughto be trained wi th randominitialisation. Then,when\\ntrainingdeeperarchitectures,weinitialisedtheﬁrstfou rconvolutionallayersandthelastthreefully-\\nconnectedlayerswiththelayersofnetA(theintermediatel ayerswereinitialisedrandomly). Wedid\\nnotdecreasethelearningrateforthepre-initialisedlaye rs,allowingthemtochangeduringlearning.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='For random initialisation (where applicable), we sampled t he weights from a normal distribution\\nwith thezeromeanand 10−2variance. The biaseswere initialisedwith zero. It isworth notingthat\\nafter the paper submission we found that it is possible to ini tialise the weights without pre-training\\nbyusingthe randominitialisationprocedureofGlorot&Ben gio(2010).\\nToobtaintheﬁxed-size 224×224ConvNetinputimages,theywererandomlycroppedfromresca led', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='training images (one crop per image per SGD iteration). To fu rther augment the training set, the\\ncropsunderwentrandomhorizontalﬂippingandrandomRGBco lourshift(Krizhevskyet al.,2012).\\nTrainingimagerescalingisexplainedbelow.\\nTraining image size. LetSbe the smallest side of an isotropically-rescaledtraining image, from\\nwhich the ConvNet input is cropped (we also refer to Sas the training scale). While the crop size', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='is ﬁxed to 224×224, in principle Scan take on any value not less than 224: forS= 224the crop\\nwill capture whole-image statistics, completely spanning the smallest side of a training image; for\\nS≫224thecropwillcorrespondtoasmallpartoftheimage,contain ingasmallobjectoranobject\\npart.\\nWe considertwoapproachesforsettingthetrainingscale S. Theﬁrst istoﬁx S,whichcorresponds\\nto single-scale training (note that image content within th e sampled crops can still represent multi-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='scale image statistics). In our experiments, we evaluated m odels trained at two ﬁxed scales: S=\\n256(which has been widely used in the prior art (Krizhevskyet al ., 2012; Zeiler&Fergus, 2013;\\nSermanetet al., 2014)) and S= 384. Given a ConvNet conﬁguration,we ﬁrst trained the network\\nusingS= 256. To speed-up training of the S= 384network, it was initialised with the weights\\npre-trainedwith S= 256,andwe useda smallerinitiallearningrateof 10−3.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='The second approachto setting Sis multi-scale training, where each training image is indiv idually\\nrescaled by randomly sampling Sfrom a certain range [Smin,Smax](we used Smin= 256and\\nSmax= 512). Sinceobjectsinimagescanbeofdifferentsize,itisbene ﬁcialtotakethisintoaccount\\nduringtraining. Thiscanalso beseen astrainingset augmen tationbyscale jittering,wherea single\\n4', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 3}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nmodel is trained to recognise objects over a wide range of sca les. For speed reasons, we trained\\nmulti-scale models by ﬁne-tuning all layers of a single-sca le model with the same conﬁguration,\\npre-trainedwithﬁxed S= 384.\\n3.2 T ESTING\\nAttest time,givena trainedConvNetandaninputimage,itis classiﬁed inthefollowingway. First,\\nit is isotropically rescaled to a pre-deﬁned smallest image side, denoted as Q(we also refer to it', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='as the test scale). We note that Qis not necessarily equal to the training scale S(as we will show\\nin Sect. 4, usingseveralvaluesof QforeachSleadsto improvedperformance). Then,the network\\nis applied densely overthe rescaled test image in a way simil ar to (Sermanetet al., 2014). Namely,\\nthe fully-connected layers are ﬁrst converted to convoluti onal layers (the ﬁrst FC layer to a 7×7\\nconv. layer, the last two FC layers to 1×1conv. layers). The resulting fully-convolutional net is', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='then applied to the whole (uncropped) image. The result is a c lass score map with the number of\\nchannels equal to the number of classes, and a variable spati al resolution, dependent on the input\\nimagesize. Finally,toobtainaﬁxed-sizevectorofclasssc oresfortheimage,theclassscoremapis\\nspatially averaged(sum-pooled). We also augmentthe test s et by horizontalﬂippingof the images;\\nthesoft-maxclassposteriorsoftheoriginalandﬂippedima gesareaveragedtoobtaintheﬁnalscores\\nfortheimage.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='fortheimage.\\nSince the fully-convolutionalnetwork is applied over the w hole image, there is no need to sample\\nmultiple crops at test time (Krizhevskyetal., 2012), which is less efﬁcient as it requires network\\nre-computationforeachcrop. Atthesametime,usingalarge setofcrops,asdonebySzegedyetal.\\n(2014),canleadtoimprovedaccuracy,asit resultsin aﬁner samplingoftheinputimagecompared\\ntothefully-convolutionalnet. Also,multi-cropevaluati oniscomplementarytodenseevaluationdue', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='to different convolution boundary conditions: when applyi ng a ConvNet to a crop, the convolved\\nfeature mapsare paddedwith zeros, while in the case of dense evaluationthe paddingfor the same\\ncrop naturally comes from the neighbouring parts of an image (due to both the convolutions and\\nspatial pooling), which substantially increases the overa ll network receptive ﬁeld, so more context\\niscaptured. Whilewebelievethatinpracticetheincreased computationtimeofmultiplecropsdoes', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='notjustifythepotentialgainsinaccuracy,forreferencew ealsoevaluateournetworksusing 50crops\\nperscale( 5×5regulargridwith 2ﬂips),foratotalof 150cropsover 3scales,whichiscomparable\\nto144cropsover 4scalesusedbySzegedyetal. (2014).\\n3.3 IMPLEMENTATION DETAILS\\nOurimplementationisderivedfromthepubliclyavailableC ++ Caffetoolbox(Jia,2013)(branched\\nout in December 2013), but contains a number of signiﬁcant mo diﬁcations, allowing us to perform', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='trainingandevaluationonmultipleGPUsinstalledinasing lesystem,aswellastrainandevaluateon\\nfull-size (uncropped) images at multiple scales (as descri bed above). Multi-GPU training exploits\\ndata parallelism, and is carried out by splitting each batch of training images into several GPU\\nbatches, processed in parallel on each GPU. After the GPU bat ch gradientsare computed, they are\\naveraged to obtain the gradient of the full batch. Gradient c omputation is synchronous across the', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='GPUs, sothe resultisexactlythesame aswhentrainingona si ngleGPU.\\nWhile more sophisticated methods of speeding up ConvNet tra ining have been recently pro-\\nposed (Krizhevsky, 2014), which employmodeland data paral lelism for differentlayersof the net,\\nwehavefoundthatourconceptuallymuchsimplerschemealre adyprovidesaspeedupof 3.75times\\non an off-the-shelf4-GPU system, as comparedto using a sing le GPU. On a system equippedwith', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='fourNVIDIATitanBlackGPUs,trainingasinglenettook2–3w eeksdependingonthearchitecture.\\n4 CLASSIFICATION EXPERIMENTS\\nDataset. In this section, we present the image classiﬁcation results achieved by the described\\nConvNetarchitecturesontheILSVRC-2012dataset(whichwa susedforILSVRC2012–2014chal-\\nlenges). The dataset includes images of 1000 classes, and is split into three sets: training ( 1.3M\\nimages), validation ( 50K images), and testing ( 100K images with held-out class labels). The clas-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='siﬁcation performanceis evaluated using two measures: the top-1 and top-5 error. The former is a\\nmulti-class classiﬁcation error, i.e. the proportion of in correctly classiﬁed images; the latter is the\\n5', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 4}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nmain evaluation criterion used in ILSVRC, and is computed as the proportion of images such that\\ntheground-truthcategoryisoutsidethetop-5predictedca tegories.\\nForthemajorityofexperiments,weusedthevalidationseta sthetestset. Certainexperimentswere\\nalso carried out on the test set and submitted to the ofﬁcial I LSVRC server as a “VGG” team entry\\ntothe ILSVRC-2014competition(Russakovskyet al., 2014).\\n4.1 SINGLESCALEEVALUATION', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='We begin with evaluating the performanceof individual Conv Net models at a single scale with the\\nlayerconﬁgurationsdescribedin Sect. 2.2. The test images ize was set as follows: Q=Sforﬁxed\\nS,andQ= 0.5(Smin+Smax)forjittered S∈[Smin,Smax]. Theresultsofareshownin Table3.\\nFirst, we note that using local response normalisation (A-L RN network) does not improve on the\\nmodel A without any normalisation layers. We thus do not empl oy normalisation in the deeper\\narchitectures(B–E).', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='architectures(B–E).\\nSecond, we observe that the classiﬁcation error decreases w ith the increased ConvNet depth: from\\n11 layers in A to 19 layers in E. Notably, in spite of the same de pth, the conﬁguration C (which\\ncontainsthree 1×1conv.layers),performsworsethantheconﬁgurationD,whic huses3×3conv.\\nlayersthroughoutthenetwork. Thisindicatesthatwhileth e additionalnon-linearitydoeshelp(Cis\\nbetter than B), it is also important to capture spatial conte xt by using conv. ﬁlters with non-trivial', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='receptive ﬁelds (D is better than C). The error rate of our arc hitecture saturates when the depth\\nreaches19layers,butevendeepermodelsmightbebeneﬁcialforlarger datasets. Wealsocompared\\nthe net B with a shallow net with ﬁve 5×5conv. layers, which was derived from B by replacing\\neachpairof 3×3conv. layerswithasingle 5×5conv. layer(whichhasthesamereceptiveﬁeldas\\nexplained in Sect. 2.3). The top-1 error of the shallow net wa s measured to be 7%higher than that', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='of B (on a center crop),which conﬁrmsthat a deepnet with smal l ﬁlters outperformsa shallow net\\nwithlargerﬁlters.\\nFinally, scale jittering at training time ( S∈[256;512] ) leads to signiﬁcantly better results than\\ntraining on images with ﬁxed smallest side ( S= 256orS= 384), even though a single scale is\\nusedattesttime. Thisconﬁrmsthattrainingsetaugmentati onbyscalejitteringisindeedhelpfulfor\\ncapturingmulti-scaleimagestatistics.\\nTable3:ConvNetperformanceatasingle testscale.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='ConvNet conﬁg. (Table 1) smallest image side top-1 val.error (%) top-5 val.error (%)\\ntrain(S)test (Q)\\nA 256 256 29.6 10.4\\nA-LRN 256 256 29.7 10.5\\nB 256 256 28.7 9.9\\nC256 256 28.1 9.4\\n384 384 28.1 9.3\\n[256;512] 384 27.3 8.8\\nD256 256 27.0 8.8\\n384 384 26.8 8.7\\n[256;512] 384 25.6 8.1\\nE256 256 27.3 9.0\\n384 384 26.9 8.7\\n[256;512] 384 25.5 8.0\\n4.2 M ULTI-SCALEEVALUATION\\nHavingevaluatedtheConvNetmodelsatasinglescale,wenow assesstheeffectofscalejitteringat', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='testtime. Itconsistsofrunningamodeloverseveralrescal edversionsofatestimage(corresponding\\nto different values of Q), followed by averaging the resulting class posteriors. Co nsidering that a\\nlarge discrepancy between training and testing scales lead s to a drop in performance, the models\\ntrained with ﬁxed Swere evaluated over three test image sizes, close to the trai ning one: Q=\\n{S−32,S,S+ 32}. At the same time, scale jittering at training time allows th e network to be', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='appliedto a widerrangeofscales at test time,so the modeltr ainedwithvariable S∈[Smin;Smax]\\nwasevaluatedoveralargerrangeofsizes Q={Smin,0.5(Smin+Smax),Smax}.\\n6', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 5}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nTheresults,presentedinTable4,indicatethatscalejitte ringattest timeleadstobetterperformance\\n(as compared to evaluating the same model at a single scale, s hown in Table 3). As before, the\\ndeepest conﬁgurations(D and E) perform the best, and scale j ittering is better than training with a\\nﬁxed smallest side S. Our best single-network performance on the validation set is24.8%/7.5%', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='top-1/top-5error(highlightedinboldinTable4). Onthete stset,theconﬁgurationEachieves 7.3%\\ntop-5error.\\nTable4:ConvNetperformanceatmultiple test scales.\\nConvNet conﬁg. (Table 1) smallest image side top-1val. error (%) top-5val. error (%)\\ntrain(S)test(Q)\\nB 256 224,256,288 28.2 9.6\\nC256 224,256,288 27.7 9.2\\n384 352,384,416 27.8 9.2\\n[256;512] 256,384,512 26.3 8.2\\nD256 224,256,288 26.6 8.6\\n384 352,384,416 26.5 8.6\\n[256;512] 256,384,512 24.8 7.5\\nE256 224,256,288 26.9 8.7\\n384 352,384,416 26.7 8.6', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='[256;512] 256,384,512 24.8 7.5\\n4.3 M ULTI-CROP EVALUATION\\nIn Table 5 we compare dense ConvNet evaluation with mult-cro p evaluation (see Sect. 3.2 for de-\\ntails). We also assess the complementarityof thetwo evalua tiontechniquesbyaveragingtheirsoft-\\nmax outputs. As can be seen, using multiple crops performs sl ightly better than dense evaluation,\\nandthe two approachesareindeedcomplementary,astheir co mbinationoutperformseach ofthem.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='As noted above, we hypothesize that this is due to a different treatment of convolution boundary\\nconditions.\\nTable 5:ConvNetevaluationtechniques comparison. Inall experimentsthe trainingscale Swas\\nsampledfrom [256;512] ,andthreetest scales Qwereconsidered: {256,384,512}.\\nConvNet conﬁg. (Table 1) Evaluationmethod top-1 val. error(%) top-5 val. error (%)\\nDdense 24.8 7.5\\nmulti-crop 24.6 7.5\\nmulti-crop &dense 24.4 7.2\\nEdense 24.8 7.5\\nmulti-crop 24.6 7.4\\nmulti-crop &dense 24.4 7.1\\n4.4 C ONVNETFUSION', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='4.4 C ONVNETFUSION\\nUpuntilnow,weevaluatedtheperformanceofindividualCon vNetmodels. Inthispartoftheexper-\\niments,wecombinetheoutputsofseveralmodelsbyaveragin gtheirsoft-maxclassposteriors. This\\nimprovesthe performancedueto complementarityof the mode ls, andwas used in the top ILSVRC\\nsubmissions in 2012 (Krizhevskyet al., 2012) and 2013 (Zeil er&Fergus, 2013; Sermanetet al.,\\n2014).\\nThe results are shown in Table 6. By the time of ILSVRC submiss ion we had only trained the', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='single-scale networks, as well as a multi-scale model D (by ﬁ ne-tuning only the fully-connected\\nlayers rather than all layers). The resulting ensemble of 7 n etworks has 7.3%ILSVRC test error.\\nAfter the submission, we considered an ensemble of only two b est-performing multi-scale models\\n(conﬁgurations D and E), which reduced the test error to 7.0%using dense evaluation and 6.8%\\nusing combined dense and multi-crop evaluation. For refere nce, our best-performingsingle model', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='achieves7.1%error(modelE, Table5).\\n4.5 C OMPARISON WITH THE STATE OF THE ART\\nFinally, we compare our results with the state of the art in Ta ble 7. In the classiﬁcation task of\\nILSVRC-2014 challenge (Russakovskyet al., 2014), our “VGG ” team secured the 2nd place with\\n7', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 6}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nTable6:Multiple ConvNetfusion results.\\nCombined ConvNet modelsError\\ntop-1 val top-5val top-5test\\nILSVRCsubmission\\n(D/256/224,256,288), (D/384/352,384,416), (D/[256;512 ]/256,384,512)\\n(C/256/224,256,288), (C/384/352,384,416)\\n(E/256/224,256,288), (E/384/352,384,416)24.7 7.5 7.3\\npost-submission\\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,dense eval. 24.0 7.1 7.0\\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop 23.9 7.2 -', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop &dense eval. 23.7 6.8 6.8\\n7.3%test errorusinganensembleof7 models. Afterthesubmissio n,we decreasedtheerrorrateto\\n6.8%usinganensembleof2models.\\nAs can be seen from Table 7, our very deep ConvNetssigniﬁcant ly outperformthe previousgener-\\nation of models, which achieved the best results in the ILSVR C-2012 and ILSVRC-2013 competi-\\ntions. Our result is also competitivewith respect to the cla ssiﬁcation task winner(GoogLeNetwith', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='6.7%error) and substantially outperforms the ILSVRC-2013 winn ing submission Clarifai, which\\nachieved 11.2%with outside training data and 11.7%without it. This is remarkable, considering\\nthat our best result is achievedby combiningjust two models – signiﬁcantly less than used in most\\nILSVRC submissions. In terms of the single-net performance , our architecture achieves the best\\nresult (7.0%test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='from the classical ConvNet architecture of LeCunetal. (198 9), but improved it by substantially\\nincreasingthedepth.\\nTable 7:Comparison with the state of the art in ILSVRC classiﬁcation . Our methodis denoted\\nas“VGG”.Onlytheresultsobtainedwithoutoutsidetrainin gdataarereported.\\nMethod top-1 val. error(%) top-5val. error (%) top-5testerror (%)\\nVGG(2nets, multi-crop& dense eval.) 23.7 6.8 6.8\\nVGG(1net, multi-crop& dense eval.) 24.4 7.1 7.0\\nVGG(ILSVRCsubmission, 7nets, dense eval.) 24.7 7.5 7.3', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='GoogLeNet (Szegedy et al., 2014) (1net) - 7.9\\nGoogLeNet (Szegedy et al., 2014) (7nets) - 6.7\\nMSRA(He et al., 2014) (11nets) - - 8.1\\nMSRA(He et al., 2014) (1net) 27.9 9.1 9.1\\nClarifai(Russakovsky et al., 2014) (multiplenets) - - 11.7\\nClarifai(Russakovsky et al., 2014) (1net) - - 12.5\\nZeiler& Fergus (Zeiler&Fergus, 2013) (6nets) 36.0 14.7 14.8\\nZeiler& Fergus (Zeiler&Fergus, 2013) (1net) 37.5 16.0 16.1\\nOverFeat (Sermanetet al.,2014) (7nets) 34.0 13.2 13.6', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='OverFeat (Sermanetet al.,2014) (1net) 35.7 14.2 -\\nKrizhevsky et al.(Krizhevsky et al., 2012) (5nets) 38.1 16.4 16.4\\nKrizhevsky et al.(Krizhevsky et al., 2012) (1net) 40.7 18.2 -\\n5 CONCLUSION\\nIn this work we evaluated very deep convolutional networks ( up to 19 weight layers) for large-\\nscale image classiﬁcation. It was demonstrated that the rep resentation depth is beneﬁcial for the\\nclassiﬁcationaccuracy,andthatstate-of-the-artperfor manceontheImageNetchallengedatasetcan', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='beachievedusingaconventionalConvNetarchitecture(LeC unet al.,1989;Krizhevskyet al.,2012)\\nwithsubstantiallyincreaseddepth. Intheappendix,weals oshowthatourmodelsgeneralisewellto\\na wide range of tasks and datasets, matchingor outperformin gmore complexrecognitionpipelines\\nbuiltaroundlessdeepimagerepresentations. Ourresultsy etagainconﬁrmtheimportanceof depth\\ninvisualrepresentations.\\nACKNOWLEDGEMENTS\\nThisworkwassupportedbyERCgrantVisRecno.228180. Wegra tefullyacknowledgethesupport', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='ofNVIDIACorporationwiththedonationoftheGPUsusedfort hisresearch.\\n8', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 7}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nREFERENCES\\nBell, S., Upchurch, P.,Snavely, N., and Bala, K. Material re cognition inthe wild withthe materials in context\\ndatabase. CoRR,abs/1412.0623, 2014.\\nChatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. R eturn of the devil in the details: Delving deep\\nintoconvolutional nets. In Proc.BMVC. ,2014.\\nCimpoi,M.,Maji,S.,andVedaldi,A. Deepconvolutionalﬁlt erbanksfortexturerecognitionandsegmentation.\\nCoRR,abs/1411.6836, 2014.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance\\nconvolutional neural networks for image classiﬁcation. In IJCAI,pp. 1237–1242, 2011.\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M. , Ranzato, M., Senior, A., Tucker, P., Yang,\\nK.,Le,Q. V.,andNg, A.Y. Large scale distributeddeepnetwo rks. InNIPS,pp. 1232–1240, 2012.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. Imagenet: A large-scale hierarchical image', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='database. In Proc.CVPR ,2009.\\nDonahue,J.,Jia,Y.,Vinyals,O.,Hoffman,J.,Zhang,N.,Tz eng,E.,andDarrell,T.Decaf: Adeepconvolutional\\nactivation feature for generic visual recognition. CoRR,abs/1310.1531, 2013.\\nEveringham, M., Eslami, S.M. A., Van Gool, L., Williams,C., Winn, J., and Zisserman, A. The Pascal visual\\nobject classes challenge: Aretrospective. IJCV,111(1):98–136, 2015.\\nFei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='incremental bayesian approach tested on 101 object categor ies. InIEEE CVPR Workshop of Generative\\nModel BasedVision , 2004.\\nGirshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection\\nand semantic segmentation. CoRR,abs/1311.2524v5, 2014. PublishedinProc.CVPR,2014.\\nGkioxari, G.,Girshick, R.,and Malik, J. Actions and attrib utes from wholes and parts. CoRR,abs/1412.2604,\\n2014.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='2014.\\nGlorot, X. andBengio, Y. Understanding the difﬁcultyof tra iningdeep feedforward neural networks. In Proc.\\nAISTATS,volume 9, pp. 249–256, 2010.\\nGoodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Sh et, V. Multi-digit number recognition from street\\nview imagery usingdeep convolutional neural networks. In Proc.ICLR ,2014.\\nGrifﬁn, G., Holub, A., and Perona, P. Caltech-256 object cat egory dataset. Technical Report 7694, California\\nInstitute of Technology, 2007.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid poolin g in deep convolutional networks for visual\\nrecognition. CoRR,abs/1406.4729v2, 2014.\\nHoai, M. Regularizedmax pooling forimage categorization. InProc. BMVC. ,2014.\\nHoward, A.G. Someimprovements ondeepconvolutional neura l networkbasedimageclassiﬁcation. In Proc.\\nICLR,2014.\\nJia, Y. Caffe: An open source convolutional architecture fo r fast feature embedding.\\nhttp://caffe.berkeleyvision.org/ ,2013.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignmen ts for generating image descriptions. CoRR,\\nabs/1412.2306, 2014.\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visu al-semantic embeddings with multimodal neural\\nlanguage models. CoRR,abs/1411.2539, 2014.\\nKrizhevsky, A. One weirdtrickfor parallelizingconvoluti onal neural networks. CoRR,abs/1404.5997, 2014.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet cl assiﬁcation with deep convolutional neural net-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='works. In NIPS,pp. 1106–1114, 2012.\\nLeCun,Y.,Boser, B.,Denker, J.S.,Henderson, D.,Howard, R .E.,Hubbard, W.,andJackel, L.D. Backpropa-\\ngationapplied tohandwrittenzipcode recognition. Neural Computation , 1(4):541–551, 1989.\\nLin,M., Chen, Q.,andYan, S. Networkinnetwork. In Proc.ICLR ,2014.\\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutiona l networks for semantic segmentation. CoRR,\\nabs/1411.4038, 2014.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations\\nusing Convolutional Neural Networks. In Proc.CVPR ,2014.\\nPerronnin, F.,S´ anchez, J.,andMensink, T. Improving theF isherkernel forlarge-scale image classiﬁcation. In\\nProc.ECCV ,2010.\\nRazavian, A.,Azizpour, H.,Sullivan, J.,andCarlsson,S. C NNFeaturesoff-the-shelf: anAstounding Baseline\\nfor Recognition. CoRR,abs/1403.6382, 2014.\\n9', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 8}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,\\nBernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large sc ale visual recognition challenge. CoRR,\\nabs/1409.0575, 2014.\\nSermanet,P.,Eigen, D.,Zhang, X.,Mathieu, M.,Fergus,R., andLeCun,Y. OverFeat: IntegratedRecognition,\\nLocalizationand Detectionusing Convolutional Networks. InProc.ICLR ,2014.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='Simonyan, K. and Zisserman, A. Two-stream convolutional ne tworks for action recognition in videos. CoRR,\\nabs/1406.2199, 2014. Published inProc.NIPS,2014.\\nSzegedy, C., Liu, W.,Jia, Y., Sermanet, P.,Reed, S.,Anguel ov, D.,Erhan, D., Vanhoucke, V., and Rabinovich,\\nA. Goingdeeper withconvolutions. CoRR,abs/1409.4842, 2014.\\nWei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan , S. CNN: Single-label to multi-label. CoRR,\\nabs/1406.5726, 2014.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='Zeiler, M. D. and Fergus, R. Visualizing and understanding c onvolutional networks. CoRR, abs/1311.2901,\\n2013. PublishedinProc. ECCV,2014.\\nA LOCALISATION\\nIn the main bodyof the paper we have consideredthe classiﬁca tion task of the ILSVRC challenge,\\nand performed a thorough evaluation of ConvNet architectur es of different depth. In this section,\\nwe turn to the localisation task of the challenge, which we ha ve won in 2014 with 25.3%error. It', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='can be seen as a special case of object detection, where a sing le object bounding box should be\\npredictedforeach ofthe top-5classes, irrespectiveof the actual numberofobjectsof the class. For\\nthiswe adoptthe approachof Sermanetet al. (2014), the winn ersof the ILSVRC-2013localisation\\nchallenge,withafewmodiﬁcations. Ourmethodisdescribed inSect.A.1andevaluatedinSect.A.2.\\nA.1 L OCALISATION CONVNET\\nTo perform object localisation, we use a very deep ConvNet, w here the last fully connected layer', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='predicts the bounding box location instead of the class scor es. A bounding box is represented by\\na 4-D vector storing its center coordinates, width, and heig ht. There is a choice of whether the\\nboundingbox prediction is shared across all classes (singl e-class regression, SCR (Sermanetet al.,\\n2014))orisclass-speciﬁc(per-classregression,PCR).In theformercase,thelastlayeris4-D,while\\nin the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='boxpredictionlayer,weuse theConvNetarchitectureD (Tab le1),whichcontains16weightlayers\\nandwasfoundtobe thebest-performingin theclassiﬁcation task (Sect.4).\\nTraining. Training of localisation ConvNets is similar to that of the c lassiﬁcation ConvNets\\n(Sect.3.1). Themaindifferenceisthatwereplacethelogis ticregressionobjectivewithaEuclidean\\nloss,whichpenalisesthedeviationofthepredictedboundi ngboxparametersfromtheground-truth.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='We trainedtwo localisation models, each on a single scale: S= 256andS= 384(due to the time\\nconstraints,we didnot use trainingscale jitteringforour ILSVRC-2014submission). Trainingwas\\ninitialised with the correspondingclassiﬁcation models ( trained on the same scales), and the initial\\nlearning rate was set to 10−3. We exploredboth ﬁne-tuningall layers and ﬁne-tuningonly the ﬁrst\\ntwo fully-connected layers, as done in (Sermanetetal., 201 4). The last fully-connected layer was', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='initialisedrandomlyandtrainedfromscratch.\\nTesting. We consider two testing protocols. The ﬁrst is used for compa ring different network\\nmodiﬁcations on the validation set, and considers only the b oundingbox prediction for the ground\\ntruth class (to factor out the classiﬁcation errors). The bo unding box is obtained by applying the\\nnetworkonlyto thecentralcropoftheimage.\\nThe second, fully-ﬂedged, testing procedure is based on the dense application of the localisation', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='ConvNet to the whole image, similarly to the classiﬁcation t ask (Sect. 3.2). The difference is that\\ninstead of the class score map, the output of the last fully-c onnected layer is a set of bounding\\nbox predictions. To come up with the ﬁnal prediction, we util ise the greedy merging procedure\\nof Sermanetetal. (2014), which ﬁrst merges spatially close predictions (by averaging their coor-\\ndinates), and then rates them based on the class scores, obta ined from the classiﬁcation ConvNet.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='When several localisation ConvNets are used, we ﬁrst take th e union of their sets of boundingbox\\npredictions, and then run the mergingprocedureon the union . We did not use the multiple pooling\\n10', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 9}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\noffsets technique of Sermanetetal. (2014), which increase s the spatial resolution of the bounding\\nboxpredictionsandcanfurtherimprovetheresults.\\nA.2 L OCALISATION EXPERIMENTS\\nIn this section we ﬁrst determine the best-performinglocal isation setting (using the ﬁrst test proto-\\ncol), and then evaluate it in a fully-ﬂedged scenario (the se cond protocol). The localisation error', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='is measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box\\npredictionis deemed correctif its intersectionoverunion ratio with the ground-truthboundingbox\\nisabove0.5.\\nSettings comparison. As can be seen from Table 8, per-class regression (PCR) outpe rforms the\\nclass-agnostic single-class regression (SCR), which diff ers from the ﬁndings of Sermanetetal.\\n(2014), where PCR was outperformed by SCR. We also note that ﬁ ne-tuning all layers for the lo-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='calisation task leads to noticeablybetter results than ﬁne -tuningonly the fully-connectedlayers(as\\ndonein(Sermanetet al.,2014)). Intheseexperiments,thes mallestimagessidewassetto S= 384;\\ntheresultswith S= 256exhibitthesamebehaviourandarenotshownforbrevity.\\nTable 8:Localisation error for different modiﬁcations with the simpliﬁed testing protocol: the\\nboundingbox is predictedfrom a single central image crop, a nd the ground-truthclass is used. All', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='ConvNet layers (except for the last one) have the conﬁgurati on D (Table 1), while the last layer\\nperformseithersingle-classregression(SCR) orper-clas sregression(PCR).\\nFine-tunedlayers regression type GTclass localisationerror\\n1st and2nd FCSCR 36.4\\nPCR 34.3\\nall PCR 33.1\\nFully-ﬂedgedevaluation. Havingdeterminedthebestlocalisationsetting(PCR,ﬁne- tuningofall\\nlayers),we nowapply it in the fully-ﬂedgedscenario,where the top-5class labelsare predictedus-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='ing our best-performingclassiﬁcation system (Sect. 4.5), and multiple densely-computedbounding\\nbox predictions are merged using the method of Sermanetetal . (2014). As can be seen from Ta-\\nble 9, applicationof the localisationConvNetto the whole i magesubstantiallyimprovesthe results\\ncompared to using a center crop (Table 8), despite using the t op-5 predicted class labels instead of\\nthegroundtruth. Similarlytotheclassiﬁcationtask(Sect .4),testingatseveralscalesandcombining', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='thepredictionsofmultiplenetworksfurtherimprovesthep erformance.\\nTable9:Localisationerror\\nsmallestimage side top-5localisationerror (%)\\ntrain(S) test(Q) val. test.\\n256 256 29.5 -\\n384 384 28.2 26.7\\n384 352,384 27.5 -\\nfusion: 256/256 and 384/352,384 26.9 25.3\\nComparison with the state of the art. We compare our best localisation result with the state\\nof the art in Table 10. With 25.3%test error, our “VGG” team won the localisation challenge of', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='ILSVRC-2014 (Russakovskyet al., 2014). Notably, our resul ts are considerably better than those\\nof the ILSVRC-2013winnerOverfeat(Sermanetet al., 2014), even thoughwe used less scales and\\ndid not employ their resolution enhancement technique. We e nvisage that better localisation per-\\nformance can be achieved if this technique is incorporated i nto our method. This indicates the\\nperformanceadvancementbroughtbyourverydeepConvNets– wegotbetterresultswithasimpler', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='localisationmethod,buta morepowerfulrepresentation.\\nB GENERALISATION OF VERYDEEPFEATURES\\nIn the previous sections we have discussed training and eval uation of very deep ConvNets on the\\nILSVRC dataset. In this section, we evaluate our ConvNets, p re-trained on ILSVRC, as feature\\n11', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 10}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nTable 10: Comparison with the state of the art in ILSVRC localisation . Our methodis denoted\\nas“VGG”.\\nMethod top-5val. error (%) top-5 testerror (%)\\nVGG 26.9 25.3\\nGoogLeNet (Szegedyet al., 2014) - 26.7\\nOverFeat (Sermanet etal.,2014) 30.0 29.9\\nKrizhevsky et al.(Krizhevsky et al.,2012) - 34.2\\nextractors on other, smaller, datasets, where training lar ge models from scratch is not feasible due', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='to over-ﬁtting. Recently, there has been a lot of interest in such a use case (Zeiler&Fergus, 2013;\\nDonahueet al., 2013; Razavianet al., 2014; Chatﬁeldet al., 2014), as it turns out that deep image\\nrepresentations,learntonILSVRC,generalisewelltoothe rdatasets,wheretheyhaveoutperformed\\nhand-crafted representations by a large margin. Following that line of work, we investigate if our\\nmodelsleadtobetterperformancethanmoreshallowmodelsu tilisedinthestate-of-the-artmethods.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='In this evaluation, we consider two models with the best clas siﬁcation performance on ILSVRC\\n(Sect.4)–conﬁgurations“Net-D”and“Net-E”(whichwemade publiclyavailable).\\nTo utilise the ConvNets, pre-trained on ILSVRC, for image cl assiﬁcation on other datasets, we\\nremove the last fully-connected layer (which performs 1000 -way ILSVRC classiﬁcation), and use\\n4096-Dactivationsofthepenultimatelayerasimagefeatur es,whichareaggregatedacrossmultiple', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='locations and scales. The resulting image descriptor is L2-normalised and combined with a linear\\nSVM classiﬁer, trained on the target dataset. For simplicit y, pre-trained ConvNet weights are kept\\nﬁxed(noﬁne-tuningisperformed).\\nAggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure\\n(Sect. 3.2). Namely, an image is ﬁrst rescaled so that its sma llest side equals Q, and then the net-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='work is densely applied over the image plane (which is possib le when all weight layers are treated\\nas convolutional). We then perform global average pooling o n the resulting feature map, which\\nproducesa 4096-Dimage descriptor. The descriptor is then a veraged with the descriptor of a hori-\\nzontally ﬂipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneﬁcial, so\\nwe extract features over several scales Q. The resulting multi-scale features can be either stacked', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='or pooled across scales. Stacking allows a subsequent class iﬁer to learn how to optimally combine\\nimage statistics over a range of scales; this, however, come s at the cost of the increased descriptor\\ndimensionality. We returntothediscussionofthisdesignc hoicein theexperimentsbelow. We also\\nassess late fusion of features, computed using two networks , which is performed by stacking their\\nrespectiveimagedescriptors.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='Table11: Comparisonwiththestateoftheartinimageclassiﬁcationo nVOC-2007,VOC-2012,\\nCaltech-101, and Caltech-256 . Our models are denoted as “VGG”. Results marked with * were\\nachievedusingConvNetspre-trainedonthe extended ILSVRCdataset(2000classes).\\nMethodVOC-2007 VOC-2012 Caltech-101 Caltech-256\\n(meanAP) (mean AP) (meanclass recall) (mean class recall)\\nZeiler& Fergus (Zeiler&Fergus, 2013) - 79.0 86.5±0.5 74.2±0.3\\nChatﬁeldetal. (Chatﬁeldet al., 2014) 82.4 83.2 88.4±0.6 77.6±0.1', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='He etal. (Heet al.,2014) 82.4 - 93.4±0.5 -\\nWeiet al.(Weiet al., 2014) 81.5(85.2∗)81.7 (90.3∗) - -\\nVGGNet-D (16layers) 89.3 89.0 91.8±1.0 85.0±0.2\\nVGGNet-E(19 layers) 89.3 89.0 92.3±0.5 85.1±0.3\\nVGGNet-D & Net-E 89.7 89.3 92.7±0.5 86.2±0.3\\nImage Classiﬁcation on VOC-2007and VOC-2012. We beginwith the evaluationon the image\\nclassiﬁcation task of PASCAL VOC-2007 and VOC-2012 benchma rks (Everinghametal., 2015).', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='These datasets contain 10K and 22.5K images respectively, a nd each image is annotated with one\\nor several labels, correspondingto 20 object categories. T he VOC organisersprovidea pre-deﬁned\\nsplit into training, validation, and test data (the test dat a for VOC-2012 is not publicly available;\\ninstead,anofﬁcialevaluationserverisprovided). Recogn itionperformanceismeasuredusingmean\\naverageprecision(mAP)acrossclasses.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='Notably, by examining the performance on the validation set s of VOC-2007 and VOC-2012, we\\nfoundthat aggregatingimage descriptors,computedat mult iple scales, by averagingperformssim-\\n12', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 11}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nilarly to the aggregation by stacking. We hypothesize that t his is due to the fact that in the VOC\\ndataset the objects appear over a variety of scales, so there is no particular scale-speciﬁc seman-\\ntics which a classiﬁer could exploit. Since averaging has a b eneﬁt of not inﬂating the descrip-\\ntor dimensionality, we were able to aggregated image descri ptors over a wide range of scales:', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='Q∈ {256,384,512,640,768}. It is worth noting though that the improvement over a smalle r\\nrangeof{256,384,512}wasrathermarginal( 0.3%).\\nThetestsetperformanceisreportedandcomparedwithother approachesinTable11. Ournetworks\\n“Net-D”and“Net-E”exhibitidenticalperformanceonVOCda tasets,andtheircombinationslightly\\nimproves the results. Our methods set the new state of the art across image representations, pre-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='trained on the ILSVRC dataset, outperformingthe previousb est result of Chatﬁeldet al. (2014) by\\nmore than 6%. It should be noted that the method of Wei et al. (2014), which achieves1%better\\nmAP on VOC-2012, is pre-trained on an extended 2000-class IL SVRC dataset, which includes\\nadditional 1000 categories, semantically close to those in VOC datasets. It also beneﬁts from the\\nfusionwith anobjectdetection-assistedclassiﬁcation pi peline.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='ImageClassiﬁcationonCaltech-101andCaltech-256. Inthissectionweevaluateverydeepfea-\\nturesonCaltech-101(Fei-Feiet al.,2004)andCaltech-256 (Grifﬁnet al.,2007)imageclassiﬁcation\\nbenchmarks. Caltech-101contains9Kimageslabelledinto1 02classes(101objectcategoriesanda\\nbackgroundclass), while Caltech-256 is larger with 31K ima ges and 257 classes. A standard eval-\\nuation protocolon these datasets is to generateseveral ran domsplits into training and test data and', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='report the average recognition performance across the spli ts, which is measured by the mean class\\nrecall(whichcompensatesforadifferentnumberoftestima gesperclass). FollowingChatﬁeld etal.\\n(2014); Zeiler&Fergus(2013); He etal. (2014),onCaltech- 101we generated3 randomsplits into\\ntraining and test data, so that each split contains 30 traini ng images per class, and up to 50 test\\nimages per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='images per class (and the rest is used for testing). In each sp lit, 20% of training images were used\\nasa validationset forhyper-parameterselection.\\nWe found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi-\\nple scales, performs better than averaging or max-pooling. This can be explained by the fact that\\nin Caltech images objects typically occupy the whole image, so multi-scale image features are se-', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='manticallydifferent(capturingthe wholeobject vs. object parts), andstacking allows a classiﬁer to\\nexploitsuchscale-speciﬁcrepresentations. We usedthree scalesQ∈ {256,384,512}.\\nOurmodelsarecomparedtoeachotherandthestateofthearti nTable11. Ascanbeseen,thedeeper\\n19-layerNet-Eperformsbetterthanthe16-layerNet-D,and theircombinationfurtherimprovesthe\\nperformance. On Caltech-101, our representations are comp etitive with the approach of He etal.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='(2014),which,however,performssigniﬁcantlyworsethano urnetsonVOC-2007. OnCaltech-256,\\nourfeaturesoutperformthestate oftheart (Chatﬁeldetal. , 2014)byalargemargin( 8.6%).\\nAction Classiﬁcation on VOC-2012. We also evaluated our best-performing image representa-\\ntion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classiﬁcation\\ntask (Everinghamet al., 2015), which consists in predictin g an action class from a single image,', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='given a bounding box of the person performing the action. The dataset contains 4.6K training im-\\nages,labelledinto11classes. SimilarlytotheVOC-2012ob jectclassiﬁcationtask,theperformance\\nis measured using the mAP. We considered two training settin gs: (i) computing the ConvNet fea-\\nturesonthewholeimageandignoringtheprovidedboundingb ox;(ii)computingthefeaturesonthe\\nwholeimageandontheprovidedboundingbox,andstackingth emtoobtaintheﬁnalrepresentation.', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='TheresultsarecomparedtootherapproachesinTable12.\\nOurrepresentationachievesthestateofartontheVOCactio nclassiﬁcationtaskevenwithoutusing\\nthe provided bounding boxes, and the results are further imp roved when using both images and\\nbounding boxes. Unlike other approaches, we did not incorpo rate any task-speciﬁc heuristics, but\\nreliedontherepresentationpowerofverydeepconvolution alfeatures.\\nOther Recognition Tasks. Since the public release of our models, they have been active ly used', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='by the research community for a wide range of image recogniti on tasks, consistently outperform-\\ning more shallow representations. For instance, Girshicke t al. (2014) achieve the state of the\\nobject detection results by replacing the ConvNet of Krizhe vskyet al. (2012) with our 16-layer\\nmodel. Similar gains over a more shallow architecture of Kri zhevskyet al. (2012) have been ob-\\n13', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 12}),\n",
       " Document(page_content='Publishedasa conferencepaperat ICLR2015\\nTable 12: Comparison with the state of the art in single-image action c lassiﬁcation on VOC-\\n2012. Our models are denoted as “VGG”. Results marked with * were a chieved using ConvNets\\npre-trainedonthe extended ILSVRCdataset (1512classes).\\nMethod VOC-2012 (meanAP)\\n(Oquab et al., 2014) 70.2∗\\n(Gkioxari etal.,2014) 73.6\\n(Hoai,2014) 76.3\\nVGG Net-D& Net-E,image-only 79.2\\nVGG Net-D& Net-E,image and bounding box 84.0', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 13}),\n",
       " Document(page_content='served in semantic segmentation (Longet al., 2014), image c aption generation (Kirosetal., 2014;\\nKarpathy& Fei-Fei, 2014),textureandmaterialrecognitio n(Cimpoiet al., 2014; Bell etal., 2014).\\nC PAPERREVISIONS\\nHere we present the list of major paper revisions, outlining the substantial changes for the conve-\\nnienceofthe reader.\\nv1Initialversion. Presentstheexperimentscarriedoutbefo rethe ILSVRCsubmission.\\nv2Addspost-submissionILSVRCexperimentswithtrainingset augmentationusingscalejittering,', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 13}),\n",
       " Document(page_content='whichimprovestheperformance.\\nv3Addsgeneralisationexperiments(AppendixB) on PASCAL VOC andCaltech image classiﬁca-\\ntiondatasets. Themodelsusedforthese experimentsarepub liclyavailable.\\nv4The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple\\ncropsforclassiﬁcation.\\nv6Camera-readyICLR-2015conferencepaper. Addsa compariso nof the net B with a shallow net\\nandtheresultsonPASCAL VOCactionclassiﬁcationbenchmar k.\\n14', metadata={'source': 'pdfs\\\\1409.1556.pdf', 'page': 13})]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ea2c9c2-cb92-442e-bbbd-7243e812266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b66efd2-3054-420d-a4f3-b84bf2a54707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:1409.1556v6  [cs.CV]  10 Apr 2015Publishedasa conferencepaperat ICLR2015\n",
      "VERYDEEPCONVOLUTIONAL NETWORKS\n",
      "FORLARGE-SCALEIMAGERECOGNITION\n",
      "KarenSimonyan∗& AndrewZisserman+\n",
      "VisualGeometryGroup,DepartmentofEngineeringScience, UniversityofOxford\n",
      "{karen,az }@robots.ox.ac.uk\n",
      "ABSTRACT\n",
      "In this work we investigate the effect of the convolutional n etwork depth on its\n",
      "accuracy in the large-scale image recognition setting. Our main contribution is\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5abf34df-3ceb-4cb2-ba93-a43af0cece13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whichimprovestheperformance.\n",
      "v3Addsgeneralisationexperiments(AppendixB) on PASCAL VOC andCaltech image classiﬁca-\n",
      "tiondatasets. Themodelsusedforthese experimentsarepub liclyavailable.\n",
      "v4The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple\n",
      "cropsforclassiﬁcation.\n",
      "v6Camera-readyICLR-2015conferencepaper. Addsa compariso nof the net B with a shallow net\n",
      "andtheresultsonPASCAL VOCactionclassiﬁcationbenchmar k.\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[118].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fddbaecf-08ac-48cf-a75b-6fa69201ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03560206-04c2-4137-be77-ea9fa63346fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2a28e00b-8598-4479-af40-20d67fe3c1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.016171867947776714,\n",
       " -0.00278568344715241,\n",
       " -0.004810338093806201,\n",
       " -0.03635182392278897,\n",
       " -0.023807526109539037,\n",
       " 0.016615801453539348,\n",
       " -0.028995208614565993,\n",
       " -0.02004043268559754,\n",
       " -0.023388959966584976,\n",
       " -0.005130604541723888,\n",
       " 0.02530421727905022,\n",
       " 0.005593563569592951,\n",
       " -0.010521226880541953,\n",
       " 0.003985889954963628,\n",
       " -0.01005826738701163,\n",
       " -0.01547425801996079,\n",
       " 0.04375917209398405,\n",
       " -0.009259187076638892,\n",
       " 0.008003489579099235,\n",
       " -0.014725912435205197,\n",
       " 0.002363946616677909,\n",
       " 0.009690436900997239,\n",
       " 0.0006052559121646146,\n",
       " -0.002707995083643949,\n",
       " -0.016970948258149453,\n",
       " 0.00015071943047532773,\n",
       " 0.007121963942614852,\n",
       " -0.015677199716396934,\n",
       " 0.004708867245588127,\n",
       " -0.02841175181938865,\n",
       " 0.003307305549254133,\n",
       " -0.00549526434021784,\n",
       " -0.005790162959665694,\n",
       " 0.0026350632170774107,\n",
       " 0.007394665886774575,\n",
       " -0.021283446501732913,\n",
       " -0.00917674268184977,\n",
       " 0.0030425307895567744,\n",
       " 0.02185421868418345,\n",
       " 0.005143288223128175,\n",
       " 0.019837491920483917,\n",
       " -0.00011227160648902415,\n",
       " -0.005764795131195861,\n",
       " -0.009836300634130315,\n",
       " -0.04454556965427502,\n",
       " 0.012493560293665227,\n",
       " -0.012277935847147314,\n",
       " -0.007661025990232154,\n",
       " -0.023883630060609794,\n",
       " 0.03833049684830809,\n",
       " 0.009303580427215156,\n",
       " 0.024593923669830008,\n",
       " -0.030314324519127085,\n",
       " -0.022640614381829384,\n",
       " 0.0036529395928110242,\n",
       " -0.006608269257806123,\n",
       " 0.0016148085928728362,\n",
       " 0.017148521660454507,\n",
       " -0.019013042384657564,\n",
       " -0.028158076328657876,\n",
       " 0.0010424512994927063,\n",
       " 0.004848389603680321,\n",
       " -9.602046506680938e-05,\n",
       " 0.009684094594633834,\n",
       " 0.01615918333504991,\n",
       " 0.010584646218885907,\n",
       " 0.026052562863805813,\n",
       " 0.002436878483244447,\n",
       " -0.00854889346741868,\n",
       " 0.012836023416871047,\n",
       " -0.00540013533270191,\n",
       " 0.007356614376900455,\n",
       " -0.01600697915819847,\n",
       " -0.0008018551276143852,\n",
       " 0.01244282556804808,\n",
       " -0.027270209317132612,\n",
       " -0.01562646312813475,\n",
       " 0.018429587452125258,\n",
       " -0.0016520673144515307,\n",
       " 0.0022735743389345315,\n",
       " 0.0246953931210643,\n",
       " -0.04221174619885572,\n",
       " -0.0059582233680869024,\n",
       " 0.02507590915112802,\n",
       " 0.019824807307757108,\n",
       " -0.0017091444628473957,\n",
       " -0.03145586888402816,\n",
       " 0.015702567079205507,\n",
       " -0.01884815359507932,\n",
       " -0.014040986188793557,\n",
       " -0.004588371340924885,\n",
       " 0.013584369001626639,\n",
       " -0.010407072816580853,\n",
       " 0.01977407258213996,\n",
       " -0.01425661156663399,\n",
       " -0.01448492062587871,\n",
       " 0.007001467572290349,\n",
       " 0.01830274877543735,\n",
       " 0.0011780095996924574,\n",
       " -0.018708632168309643,\n",
       " -0.006383131351743105,\n",
       " 0.005859924138711791,\n",
       " -0.023312857878159256,\n",
       " -0.007838599858198467,\n",
       " -0.0069443905403097994,\n",
       " 0.0007542907984793928,\n",
       " -0.015068375558411016,\n",
       " -0.001744024935955129,\n",
       " 0.009411393116135373,\n",
       " -0.016019661908280233,\n",
       " 0.0020500220094623644,\n",
       " 0.019190615786962618,\n",
       " 0.001261247132022951,\n",
       " -0.02595109154992648,\n",
       " 0.005194023414406582,\n",
       " 0.0011764241395169216,\n",
       " 0.0014562608290895282,\n",
       " -0.010413414191621737,\n",
       " -0.014193192228290038,\n",
       " -0.006950732381011943,\n",
       " 0.014624442052648384,\n",
       " 0.0102294994142758,\n",
       " 0.009982164367263392,\n",
       " -0.02212057878764103,\n",
       " 0.022640614381829384,\n",
       " 0.007622974480358035,\n",
       " -0.022361571528290037,\n",
       " -0.008846963240048236,\n",
       " 0.01686947880691516,\n",
       " 0.00026398197140193333,\n",
       " 0.02085219760869709,\n",
       " 0.009030878017394172,\n",
       " -0.00929089674581087,\n",
       " -0.0008204844884037325,\n",
       " 0.0017567088501900456,\n",
       " 0.02004043268559754,\n",
       " -0.011897421679728635,\n",
       " 0.021575173967999066,\n",
       " -0.002486028330762633,\n",
       " -0.013863412786488505,\n",
       " -0.004699354717365543,\n",
       " 0.023807526109539037,\n",
       " 0.01785881526967472,\n",
       " -0.02110587309942786,\n",
       " -0.014535655351495856,\n",
       " 0.006427524702319368,\n",
       " 0.019127198311263704,\n",
       " -0.0175544050533268,\n",
       " 0.0007019700306101353,\n",
       " -0.020598520255321273,\n",
       " 0.014307346292251137,\n",
       " -0.013964884100367838,\n",
       " 0.02083951299597028,\n",
       " 0.024657343008173963,\n",
       " -0.0017218282606669972,\n",
       " 0.017744661205713618,\n",
       " -0.005355741982125646,\n",
       " 0.03320623740691516,\n",
       " 0.005314519784731085,\n",
       " -0.013203854834207959,\n",
       " -0.02825954577989217,\n",
       " -0.010064609693375035,\n",
       " 0.030644103960928617,\n",
       " -0.010261209083447777,\n",
       " -0.017909551857936905,\n",
       " 0.03143049965857455,\n",
       " 0.03026358979350994,\n",
       " 0.0007626145517124421,\n",
       " 0.0009346388434031198,\n",
       " 0.005390622804479325,\n",
       " 0.010039242330566462,\n",
       " 0.002284672676578597,\n",
       " -0.047462848042226634,\n",
       " 0.013825361742275645,\n",
       " -0.016425543438507488,\n",
       " -0.009880693984706578,\n",
       " 0.0065385080787600264,\n",
       " -0.006627294779912553,\n",
       " -0.028208811054275022,\n",
       " -0.02417535752687595,\n",
       " -0.024834916410479014,\n",
       " 0.0046898417234816974,\n",
       " 0.03287645610246859,\n",
       " 0.030999250765538725,\n",
       " 0.0009877522831524875,\n",
       " -0.0032755961129127867,\n",
       " 0.008460106766266153,\n",
       " 0.0019342822524950987,\n",
       " 0.04127314446171331,\n",
       " -0.003947838445089509,\n",
       " 0.02349043128046431,\n",
       " 0.019685285880987436,\n",
       " 0.008143011005868906,\n",
       " -0.021537123855108725,\n",
       " -0.6798526248383211,\n",
       " -0.02234888691556323,\n",
       " 0.023021130411893104,\n",
       " -0.009018194335989885,\n",
       " 0.0034182889256947914,\n",
       " 0.019228667762497997,\n",
       " 0.01883546898235251,\n",
       " 0.020408264102934455,\n",
       " -0.0187720515066536,\n",
       " 0.01047683352996569,\n",
       " 0.00019323003981718502,\n",
       " 0.020902932334314235,\n",
       " 0.00917674268184977,\n",
       " -0.007242460312939355,\n",
       " -0.0056284443919466295,\n",
       " -0.018366168113781306,\n",
       " 0.0008664632991555316,\n",
       " -0.0062087291026197535,\n",
       " -0.022945026460822342,\n",
       " 0.002368703113619831,\n",
       " -0.012487218918624345,\n",
       " 0.009366999765559108,\n",
       " -0.01555036103970903,\n",
       " -0.012874075392406427,\n",
       " 0.016209919923312093,\n",
       " -0.001358754038763897,\n",
       " 0.014142457502672892,\n",
       " 0.013305325216764773,\n",
       " -0.0340433659675332,\n",
       " 0.02387094544788299,\n",
       " -0.023312857878159256,\n",
       " 0.011650086632716225,\n",
       " 0.014992272538662777,\n",
       " -0.018213962074284825,\n",
       " 0.0571786504433874,\n",
       " -0.007730787169278251,\n",
       " -0.002830076797728673,\n",
       " 0.0293503554191761,\n",
       " -0.0020103851558280236,\n",
       " 0.04190733411986276,\n",
       " 0.005339887613200919,\n",
       " -0.004991082183631696,\n",
       " 0.003732213532910336,\n",
       " -0.025215430577897695,\n",
       " 0.0187720515066536,\n",
       " -0.012430141886643794,\n",
       " 0.00822545633198055,\n",
       " -0.012499902600028631,\n",
       " 0.02349043128046431,\n",
       " 0.003459511355919983,\n",
       " 0.02024337345071117,\n",
       " 0.021270761889006107,\n",
       " -0.007242460312939355,\n",
       " -0.005086211191147626,\n",
       " 0.004169805197970825,\n",
       " 0.004924492157767301,\n",
       " 0.020788778270353133,\n",
       " -0.0009639701476888201,\n",
       " 0.015157162259563543,\n",
       " 0.0008973801218244253,\n",
       " 0.0009520790799569864,\n",
       " 0.009538230861500758,\n",
       " -0.011155418401336445,\n",
       " 0.00917674268184977,\n",
       " -0.013444846643534446,\n",
       " 0.0003192754916522348,\n",
       " -0.008085933973888357,\n",
       " -0.006170677592745634,\n",
       " 0.012341355185491268,\n",
       " -0.0033548698201814682,\n",
       " 0.01977407258213996,\n",
       " 0.00624043830613047,\n",
       " -0.013964884100367838,\n",
       " 0.014180508546885752,\n",
       " 0.01728804308722418,\n",
       " 0.04178049544317485,\n",
       " 0.01276626270348621,\n",
       " -0.014916170450237057,\n",
       " 0.01872131491839141,\n",
       " -0.006620952939210409,\n",
       " 0.017757345818440427,\n",
       " 0.007572239289079628,\n",
       " -0.01671727276741868,\n",
       " -0.0052352460774624035,\n",
       " 0.013723891359718831,\n",
       " -0.03827976212269094,\n",
       " -0.012360380241936437,\n",
       " 0.02058583750523951,\n",
       " 0.010965160386304585,\n",
       " 0.012455509249452367,\n",
       " 0.026838958561451746,\n",
       " 0.037747041915775784,\n",
       " 0.006249951300014316,\n",
       " -0.02772682557297701,\n",
       " 0.005726743621321741,\n",
       " -0.026230136266110867,\n",
       " -0.012461851555815771,\n",
       " 0.012106704751205665,\n",
       " 0.013939515806236745,\n",
       " -0.017529035827873185,\n",
       " 0.0019818466398377485,\n",
       " -0.005948710839864317,\n",
       " 0.012157439476822811,\n",
       " 0.019013042384657564,\n",
       " 0.0038495387500531372,\n",
       " -0.013178486540076867,\n",
       " -0.024454400380415294,\n",
       " 0.016247970036202434,\n",
       " 0.018822786232270745,\n",
       " -0.00643069585550107,\n",
       " 0.0069443905403097994,\n",
       " -0.004401284944735987,\n",
       " -0.004100044018924729,\n",
       " -0.0034721952701548997,\n",
       " 0.023553850618808263,\n",
       " -0.02633160571734516,\n",
       " -0.004724722080174116,\n",
       " 0.0026223793028424943,\n",
       " 0.030542634509694324,\n",
       " -0.016235287286120666,\n",
       " 0.009722146570169215,\n",
       " -0.006383131351743105,\n",
       " 0.023629952707233982,\n",
       " -0.005663324748639047,\n",
       " -0.013203854834207959,\n",
       " 0.02780292952404777,\n",
       " -0.024949070474440116,\n",
       " 0.0064560632183096435,\n",
       " -0.02488565113609616,\n",
       " 0.005520631703026413,\n",
       " 0.021169292437771815,\n",
       " -0.005168656051598009,\n",
       " 0.009100639662101529,\n",
       " -0.013901464762023885,\n",
       " 0.027092635914827558,\n",
       " -0.006243609459312172,\n",
       " 0.009734830251573501,\n",
       " -0.005327203466135372,\n",
       " 0.017884182632483293,\n",
       " -0.013838045423679932,\n",
       " -0.002238693982242113,\n",
       " -0.018733999531118217,\n",
       " 0.0045915420284453265,\n",
       " -0.00011514528216101159,\n",
       " 0.00849181643543813,\n",
       " 0.0009021365605586903,\n",
       " -0.007045860922866612,\n",
       " -0.02058583750523951,\n",
       " -0.006360934676454973,\n",
       " -0.003960522126493796,\n",
       " -0.007312221026324192,\n",
       " 0.007128305783316995,\n",
       " -0.013825361742275645,\n",
       " 0.026001826275543625,\n",
       " 0.03158270756071607,\n",
       " -0.007217092484469522,\n",
       " -0.0022656469216415375,\n",
       " -0.025595944745316374,\n",
       " -0.007381982205370288,\n",
       " -0.012360380241936437,\n",
       " 0.007337588854794025,\n",
       " 0.013939515806236745,\n",
       " -0.02597645891273505,\n",
       " -0.0027428756731669975,\n",
       " 0.0008046297120254017,\n",
       " -0.03148123810948178,\n",
       " -0.0006266598573649784,\n",
       " 0.026686752521955265,\n",
       " -0.0024923701714647762,\n",
       " -0.036833805678796905,\n",
       " 0.013584369001626639,\n",
       " -0.01607039663389738,\n",
       " -0.007711761181510561,\n",
       " 0.0016996315853788657,\n",
       " 0.01660311870345758,\n",
       " 0.009468470148115922,\n",
       " -0.009848984315534602,\n",
       " 0.007369298058304742,\n",
       " 0.015144478578159256,\n",
       " -0.012836023416871047,\n",
       " -0.0041095570128085734,\n",
       " -0.002503468509108842,\n",
       " -0.020788778270353133,\n",
       " 0.014320030904977944,\n",
       " 0.017567087803408567,\n",
       " 0.00833961039594165,\n",
       " 0.03995402669450718,\n",
       " 0.038761747603988955,\n",
       " -0.008345952702305053,\n",
       " 0.015119111215350683,\n",
       " 0.013064332476115766,\n",
       " 0.0011946571061585561,\n",
       " -0.025545210019699227,\n",
       " -0.0013103969795411367,\n",
       " 0.00652582439735574,\n",
       " -0.008117643643060333,\n",
       " -0.020674624206392034,\n",
       " 0.010527569186905356,\n",
       " 0.01589282323159233,\n",
       " 0.01437076563059509,\n",
       " 0.03059336923531147,\n",
       " 0.0019089146568558953,\n",
       " 0.02874153126119018,\n",
       " -0.007001467572290349,\n",
       " 0.003045701709907846,\n",
       " -0.010445123860793713,\n",
       " -0.0020674624206392035,\n",
       " -0.017985653946362624,\n",
       " 0.009449444160348233,\n",
       " 0.038381231573925235,\n",
       " 0.008345952702305053,\n",
       " 0.0024384640598352977,\n",
       " -0.009049904005161863,\n",
       " -0.0043886012633317,\n",
       " -0.00046177025287952696,\n",
       " 0.011960840086750068,\n",
       " 0.001040073051021745,\n",
       " 0.006341909154348543,\n",
       " -0.024670025758255727,\n",
       " 0.008961117304009336,\n",
       " 0.015309368299060022,\n",
       " -0.007546871926271055,\n",
       " 0.005904317489288054,\n",
       " -0.028538590496076554,\n",
       " -0.025710098809277472,\n",
       " 0.017059734959301978,\n",
       " 0.01683142683137978,\n",
       " 0.029020575977374567,\n",
       " -0.01622260267339386,\n",
       " -0.01230964551631929,\n",
       " -0.03713821775778986,\n",
       " 0.024999805200057262,\n",
       " 0.01486543479329739,\n",
       " 0.003177296417876415,\n",
       " 0.0005247929642451375,\n",
       " -0.001026596464906718,\n",
       " 0.020141902136831834,\n",
       " -0.008872330602856809,\n",
       " 0.03201395831639694,\n",
       " -0.00532403277861493,\n",
       " 0.0007511198199206636,\n",
       " 0.009132349331273507,\n",
       " 0.02749851744505481,\n",
       " -0.03523564505805143,\n",
       " 0.028716163898381608,\n",
       " 0.00520987824899257,\n",
       " 0.02276745305851729,\n",
       " -0.010426098804348544,\n",
       " 0.008466448141307037,\n",
       " 0.012087678763437975,\n",
       " -0.01740219901383032,\n",
       " 0.004572516506338897,\n",
       " -0.010812955278130627,\n",
       " 0.006557534066527717,\n",
       " 0.006199216108735909,\n",
       " -0.019647233905452057,\n",
       " -0.005060843362677792,\n",
       " 0.0010472076800193137,\n",
       " 0.0015204727228982768,\n",
       " 0.01722462561152527,\n",
       " 0.012607715288948848,\n",
       " 0.02246304097952433,\n",
       " 0.016970948258149453,\n",
       " -0.01036267946600459,\n",
       " 0.001659994731744525,\n",
       " 0.010343653478236901,\n",
       " 0.004721551392653674,\n",
       " -0.028766898623998755,\n",
       " -0.0065385080787600264,\n",
       " -0.0021070992742735443,\n",
       " -0.0017028026221452524,\n",
       " -0.004014428470953904,\n",
       " 0.0187720515066536,\n",
       " -0.006180190120968218,\n",
       " 0.03320623740691516,\n",
       " 0.011288598453065235,\n",
       " 0.005415990167287898,\n",
       " -0.024467084993142103,\n",
       " -0.012030601731457424,\n",
       " 0.006310199485176567,\n",
       " -0.013419479280725873,\n",
       " -0.027371678768366905,\n",
       " 0.014446868650343331,\n",
       " 0.0353371182345758,\n",
       " -0.0132799578539562,\n",
       " -0.015119111215350683,\n",
       " -0.02106782112389248,\n",
       " 0.003732213532910336,\n",
       " -0.009734830251573501,\n",
       " 0.004838876609796476,\n",
       " -0.011066630768861399,\n",
       " 0.0022180826507142023,\n",
       " -0.002387728635726261,\n",
       " -0.0003297792671785603,\n",
       " 0.012017917118730617,\n",
       " 0.010584646218885907,\n",
       " 0.04452020042882141,\n",
       " 0.010755877314827557,\n",
       " -0.006395815033147392,\n",
       " 0.009468470148115922,\n",
       " -0.003611717162585833,\n",
       " 0.003411947084992648,\n",
       " 0.004661303207491423,\n",
       " -0.03437314727197977,\n",
       " -0.009550914542905044,\n",
       " 0.00019223911265556074,\n",
       " -0.014814699136357723,\n",
       " -0.00891038257839219,\n",
       " -0.014966905175854204,\n",
       " 0.006148480917457502,\n",
       " -0.0021118557712154666,\n",
       " 0.001414245726984226,\n",
       " -0.022868924372396623,\n",
       " -0.013888781080619599,\n",
       " 0.0021784457970798615,\n",
       " -0.005656982907936904,\n",
       " -0.008352294077345937,\n",
       " -0.011117366425801066,\n",
       " 0.03670697072739908,\n",
       " -0.033815057839611,\n",
       " -0.0054191613204696,\n",
       " -0.021245394526197534,\n",
       " -0.03678307095317976,\n",
       " -0.009449444160348233,\n",
       " 0.06428158653558952,\n",
       " 0.00622141278402404,\n",
       " 0.025266165303514842,\n",
       " 0.006532166238057883,\n",
       " 0.009468470148115922,\n",
       " 0.007286853663515619,\n",
       " -0.007921044252987591,\n",
       " -0.011941815030304897,\n",
       " -0.010330969796832614,\n",
       " -0.01053391056194624,\n",
       " -0.009373341140599992,\n",
       " -0.036022042618342395,\n",
       " -0.01694558089534088,\n",
       " 0.007045860922866612,\n",
       " 0.0010408658393171704,\n",
       " -0.008155694687273193,\n",
       " 0.006440208383723655,\n",
       " -0.026915062512522504,\n",
       " -0.004353720440978022,\n",
       " 0.007673709671636441,\n",
       " -6.758096757793353e-05,\n",
       " 0.0047786284246342245,\n",
       " 0.017161206273181313,\n",
       " 0.021841534071456645,\n",
       " 0.01270918567150566,\n",
       " 0.023274805902623878,\n",
       " 0.012030601731457424,\n",
       " -0.0015466331068329055,\n",
       " 0.005676008430043334,\n",
       " -0.00414126668198055,\n",
       " 0.0031233900734163066,\n",
       " 0.0018756196439236978,\n",
       " 0.01230964551631929,\n",
       " 0.0009259186960223577,\n",
       " 0.010096319362547011,\n",
       " 0.018886205570614698,\n",
       " -0.00040211665893884446,\n",
       " 0.006437037696203214,\n",
       " 0.005863094826232232,\n",
       " -0.006215070943321897,\n",
       " 0.026052562863805813,\n",
       " 0.03087241395149586,\n",
       " 0.0013230807773607384,\n",
       " -0.01875936689392679,\n",
       " 0.012030601731457424,\n",
       " -0.007318562867026335,\n",
       " -0.02576083353489462,\n",
       " 0.002387728635726261,\n",
       " -0.007521504097801221,\n",
       " 0.0005461969094455012,\n",
       " 0.03401800046736966,\n",
       " 0.0030155776173267203,\n",
       " -0.028310282368154357,\n",
       " -0.012899442755215,\n",
       " 0.0001829244322608871,\n",
       " 0.0038685645049901973,\n",
       " -0.025621312108124947,\n",
       " -0.013254589559825105,\n",
       " -0.003411947084992648,\n",
       " -0.012563321938372583,\n",
       " -0.0121764654645905,\n",
       " 0.002176860220489011,\n",
       " -0.014611758371244097,\n",
       " -0.009912403653878554,\n",
       " -0.013812678060871358,\n",
       " -0.0441650536242113,\n",
       " -0.0034182889256947914,\n",
       " 0.01452297167009157,\n",
       " -0.011535932568755125,\n",
       " -0.011992549755922044,\n",
       " -0.006047010069239429,\n",
       " -0.025963776162653288,\n",
       " -0.02261524701902081,\n",
       " 0.004997424489995099,\n",
       " 0.018099808010323726,\n",
       " 0.009354316084154822,\n",
       " 0.007312221026324192,\n",
       " -0.021270761889006107,\n",
       " 0.022627931631747616,\n",
       " 0.019964328734526783,\n",
       " -0.009557256849268449,\n",
       " -0.03214079326779476,\n",
       " 0.016729955517500446,\n",
       " 0.009087955980697242,\n",
       " -0.00683023601068744,\n",
       " 0.00917674268184977,\n",
       " -0.016691905404610106,\n",
       " 0.024923703111631543,\n",
       " 0.005485751346333995,\n",
       " 0.018023705921898007,\n",
       " 0.020281425426246547,\n",
       " -0.004471045658120823,\n",
       " 0.021384917815612248,\n",
       " -0.008802569889471973,\n",
       " -0.004455191289196095,\n",
       " 0.01671727276741868,\n",
       " 0.021892270659718834,\n",
       " -0.0003811091078861935,\n",
       " -0.02732094404274976,\n",
       " 0.015563044721113315,\n",
       " 0.01265210863952511,\n",
       " -0.027625356121742717,\n",
       " 0.0016052957154043062,\n",
       " -0.02874153126119018,\n",
       " -0.018239329437093398,\n",
       " 0.027625356121742717,\n",
       " -0.024987120587330453,\n",
       " 0.021702012644686974,\n",
       " -0.021892270659718834,\n",
       " -0.009094297355738126,\n",
       " 0.0204589988285516,\n",
       " -0.043809906819601195,\n",
       " 0.02125807913892434,\n",
       " -0.007540529619907651,\n",
       " 0.02950256145867258,\n",
       " 0.023807526109539037,\n",
       " 0.015524993676900457,\n",
       " 0.0265599157079124,\n",
       " -0.016387493325617147,\n",
       " -0.014700545072396624,\n",
       " -0.0009655556660720135,\n",
       " -0.05946173917318956,\n",
       " 0.01978675533222173,\n",
       " -0.011675454926847318,\n",
       " -0.007661025990232154,\n",
       " -0.009271870758043178,\n",
       " -0.003665623507045941,\n",
       " -0.00639264434562695,\n",
       " -0.01100955373688085,\n",
       " -0.009398709434731086,\n",
       " -0.008675731212784065,\n",
       " 0.026204768903302294,\n",
       " -0.009823616952726029,\n",
       " -0.02092829969712281,\n",
       " -0.017465616489529233,\n",
       " -0.031151456805035203,\n",
       " 0.00942407679753966,\n",
       " 0.025557892769780995,\n",
       " -0.003465853196622126,\n",
       " -0.008085933973888357,\n",
       " -0.03287645610246859,\n",
       " -0.0032121770073994635,\n",
       " -0.003992231795665772,\n",
       " -0.027168738003253277,\n",
       " -0.024860283773287587,\n",
       " -0.04244005432677792,\n",
       " -0.0058504111448279454,\n",
       " 0.010299260127660637,\n",
       " -0.02272940108298191,\n",
       " 0.003710016857622204,\n",
       " -0.010007532661394484,\n",
       " 0.007128305783316995,\n",
       " -0.027523884807863382,\n",
       " 0.0002725038926050103,\n",
       " -0.019114513698536898,\n",
       " -0.020712674319282375,\n",
       " 0.007838599858198467,\n",
       " -0.0014697374152045552,\n",
       " 0.017148521660454507,\n",
       " 0.012144755795418525,\n",
       " 0.04005549614574147,\n",
       " 0.007540529619907651,\n",
       " 0.01055293654971393,\n",
       " 0.00593602669279877,\n",
       " -0.021245394526197534,\n",
       " -0.016324073987273195,\n",
       " -0.007502478575694791,\n",
       " 0.00099250878009441,\n",
       " -0.012411115898876104,\n",
       " 0.017199256386071653,\n",
       " 0.01634944135008177,\n",
       " 0.022424990866633993,\n",
       " 0.010692458907806124,\n",
       " 0.023439696554847164,\n",
       " 0.0006119942052221282,\n",
       " -0.0030552147037916913,\n",
       " -0.007039519082164469,\n",
       " -0.011504222899583149,\n",
       " -0.021537123855108725,\n",
       " -0.012411115898876104,\n",
       " 0.010546594243350527,\n",
       " -0.0028237349570265297,\n",
       " -0.018531058766004593,\n",
       " 0.01588014048151056,\n",
       " 0.0004463118415450801,\n",
       " -0.023211386564279922,\n",
       " 0.006395815033147392,\n",
       " 0.0057521114497915745,\n",
       " 0.017719293842905045,\n",
       " -0.002855444626198506,\n",
       " 0.032774986651234295,\n",
       " -0.013914148443428172,\n",
       " -0.00665900444908453,\n",
       " -0.011510565205946552,\n",
       " 0.018277381412628777,\n",
       " 0.0038495387500531372,\n",
       " -0.013292641535360486,\n",
       " -0.007724445328576108,\n",
       " 0.011910105361132921,\n",
       " 0.02576083353489462,\n",
       " -3.443458426123539e-05,\n",
       " 0.02276745305851729,\n",
       " -0.011282256146701832,\n",
       " 0.01005826738701163,\n",
       " 0.008726466869723733,\n",
       " 0.015537677358304743,\n",
       " -0.0023671175370289805,\n",
       " -0.009538230861500758,\n",
       " -0.0016806058304418058,\n",
       " -0.011218836808357878,\n",
       " 0.000452257375410997,\n",
       " -0.021308813864541486,\n",
       " -0.026077930226614386,\n",
       " -0.01971065324379601,\n",
       " 0.014738596116609484,\n",
       " 0.0013135678998922084,\n",
       " -0.0033104764696052047,\n",
       " 0.03665623227649185,\n",
       " -0.003795632638423659,\n",
       " -0.027371678768366905,\n",
       " -0.023959732149035514,\n",
       " -4.263447523706031e-05,\n",
       " 0.011377385154217762,\n",
       " -0.02299576118643949,\n",
       " 0.019697968631069204,\n",
       " -0.004842047297316917,\n",
       " -0.011225179114721283,\n",
       " 0.0036941620230362157,\n",
       " 0.004867415125786751,\n",
       " 0.0025066394294599136,\n",
       " -0.01251892858779632,\n",
       " 0.013203854834207959,\n",
       " 0.005603076563476796,\n",
       " 0.008770860220299997,\n",
       " -0.014320030904977944,\n",
       " 0.007337588854794025,\n",
       " -0.0012961276051306843,\n",
       " 0.0032597412783267983,\n",
       " -0.007191724655999689,\n",
       " 0.022716718332900145,\n",
       " 0.0167933748558444,\n",
       " -0.001948551510490236,\n",
       " -0.007578581129781771,\n",
       " -0.008047882929675497,\n",
       " -0.023731424021113317,\n",
       " 0.020699991569200608,\n",
       " 0.012277935847147314,\n",
       " 0.000998057948916443,\n",
       " -0.015398155000212549,\n",
       " -0.006253122453196017,\n",
       " -0.004353720440978022,\n",
       " 0.0204589988285516,\n",
       " -0.020814145633161706,\n",
       " -0.006189703114852064,\n",
       " 0.026762856473026026,\n",
       " 0.0032660831190289416,\n",
       " 0.019165248424154045,\n",
       " -0.006008958559365308,\n",
       " 0.016032346521007042,\n",
       " 0.006665346289786673,\n",
       " -0.0036148880829369044,\n",
       " 0.019647233905452057,\n",
       " -0.011726189652464464,\n",
       " 0.011567642237927101,\n",
       " -0.014802015454953436,\n",
       " -0.001883546944801377,\n",
       " -0.03135439943279387,\n",
       " -0.01686947880691516,\n",
       " -0.005866265979413934,\n",
       " 0.03026358979350994,\n",
       " -0.015347419343272882,\n",
       " -0.011206153126953592,\n",
       " -0.02030679278905512,\n",
       " 0.02261524701902081,\n",
       " 0.016387493325617147,\n",
       " 0.026585283070720972,\n",
       " -0.03059336923531147,\n",
       " 0.003976376961079784,\n",
       " 0.005352571294605205,\n",
       " -0.0003059971026110641,\n",
       " 0.017579772416135373,\n",
       " -0.01040073051021745,\n",
       " -0.009620676187612401,\n",
       " -0.005003766330697242,\n",
       " -0.01574061905474089,\n",
       " -0.022602564268939043,\n",
       " -0.027473150082246236,\n",
       " -0.0030615565444938346,\n",
       " 0.0035926914076487727,\n",
       " -0.007743470850682538,\n",
       " 0.0010709899318982962,\n",
       " -0.0181251753731323,\n",
       " 0.016628486066266154,\n",
       " 0.021613225943534445,\n",
       " 0.01049585951773338,\n",
       " -0.008961117304009336,\n",
       " -0.0036941620230362157,\n",
       " -0.02749851744505481,\n",
       " 0.0057996759535495395,\n",
       " 0.02409925357580519,\n",
       " 0.00979190728355405,\n",
       " 0.011206153126953592,\n",
       " -0.02239962350382542,\n",
       " 0.004750089908643949,\n",
       " -0.003074240458728751,\n",
       " -0.03264814797454639,\n",
       " 0.009937771016687127,\n",
       " 0.005748940296609872,\n",
       " -0.007724445328576108,\n",
       " -0.01626065464892924,\n",
       " 0.0023306516037457114,\n",
       " 0.020078484661132923,\n",
       " -0.0016996315853788657,\n",
       " -0.029223516742488194,\n",
       " 0.003235959026447816,\n",
       " 0.010210473426508111,\n",
       " -0.00034028304270488583,\n",
       " -0.005460383517864161,\n",
       " -0.018011021309171198,\n",
       " 0.006230925777907886,\n",
       " -0.01822664668701163,\n",
       " 0.014472236013151904,\n",
       " 0.00833961039594165,\n",
       " -0.010806612971767222,\n",
       " 0.00892940763483736,\n",
       " 0.01002021634279877,\n",
       " 0.021004401785548528,\n",
       " 0.014434184968939045,\n",
       " -0.012728211659273351,\n",
       " -0.0001700424373674291,\n",
       " -0.003884419339576186,\n",
       " 0.0013389354955314116,\n",
       " 0.0026667726534187574,\n",
       " -0.0208014610204349,\n",
       " -0.015144478578159256,\n",
       " -0.010635380944503054,\n",
       " -0.0216766452818784,\n",
       " 0.00824448138842572,\n",
       " 0.007737129009980395,\n",
       " -0.01509374385254211,\n",
       " 0.019761387969413156,\n",
       " 0.017376829788376704,\n",
       " -0.01618455256050352,\n",
       " 0.009322606414982846,\n",
       " -0.0013064332708946397,\n",
       " -0.029147414654062474,\n",
       " -0.015905507844319135,\n",
       " -0.024264144228028476,\n",
       " -0.017047052209220214,\n",
       " -0.009151374387718675,\n",
       " -0.017871499882401526,\n",
       " 0.019723337856522815,\n",
       " 0.010920767035728323,\n",
       " -0.009411393116135373,\n",
       " -0.04094336315726674,\n",
       " 0.0012509415826743105,\n",
       " -0.026382342305607345,\n",
       " 0.003526101381784378,\n",
       " -0.00942407679753966,\n",
       " 0.010096319362547011,\n",
       " -0.013406795599321586,\n",
       " -0.012094020138478857,\n",
       " -0.017833447906866147,\n",
       " 0.023997784124570896,\n",
       " 0.007115622101912709,\n",
       " -0.00671608148106508,\n",
       " -8.78453523631951e-05,\n",
       " -0.003710016857622204,\n",
       " -0.014624442052648384,\n",
       " 0.01562646312813475,\n",
       " 0.010939793023496012,\n",
       " -0.008827937252280546,\n",
       " -0.03815292344600303,\n",
       " 0.008269849682556814,\n",
       " 0.02175274737030412,\n",
       " -0.044723143056580074,\n",
       " -0.02099171903546676,\n",
       " 0.012315986891360174,\n",
       " -0.011554958556522815,\n",
       " -0.01849300679046921,\n",
       " -0.00784494123323935,\n",
       " -0.005593563569592951,\n",
       " 0.006421182861617225,\n",
       " 0.014446868650343331,\n",
       " 0.025456423318546702,\n",
       " -0.012119388432609951,\n",
       " -0.016844109581461545,\n",
       " 0.013647788339970593,\n",
       " -0.020332160151863694,\n",
       " 0.0011478855071113316,\n",
       " 0.0011795950598679932,\n",
       " 0.03439851277214331,\n",
       " -0.010089977056183608,\n",
       " 0.025659364083660326,\n",
       " -0.008428397097094177,\n",
       " 0.006798526341515463,\n",
       " 0.016653853429074727,\n",
       " -0.013647788339970593,\n",
       " 0.016032346521007042,\n",
       " -0.007312221026324192,\n",
       " -0.013191171152803672,\n",
       " 0.012512586281432918,\n",
       " 0.005913830017510639,\n",
       " -0.005029133693505815,\n",
       " -0.0022434502463534057,\n",
       " 0.018429587452125258,\n",
       " -0.02228546757721928,\n",
       " 0.011022237418285136,\n",
       " 0.020699991569200608,\n",
       " -0.008200088037849457,\n",
       " -0.00412224069421286,\n",
       " 0.005200365255108725,\n",
       " -0.0010908083587154666,\n",
       " -0.022361571528290037,\n",
       " -0.02228546757721928,\n",
       " -0.011910105361132921,\n",
       " -0.022906974485286963,\n",
       " -0.012398432217471817,\n",
       " 0.011034922031011942,\n",
       " 0.019393558414721283,\n",
       " -0.007343930695496169,\n",
       " -0.00013516193515586707,\n",
       " -0.012474535237220058,\n",
       " -0.008751834232532306,\n",
       " 0.003532443222486521,\n",
       " -0.01308969983892434,\n",
       " 0.0331301334558444,\n",
       " -0.0057204017806195976,\n",
       " -0.020218006087902595,\n",
       " 0.019000359634575796,\n",
       " -0.0107241685769781,\n",
       " -0.02663601779633812,\n",
       " 0.012290619528551601,\n",
       " 0.009931429641646245,\n",
       " -0.0001565658803562308,\n",
       " 0.015702567079205507,\n",
       " 0.24576166329599503,\n",
       " -0.0036973329433872873,\n",
       " -0.007090254273442876,\n",
       " 0.03026358979350994,\n",
       " 0.013939515806236745,\n",
       " 0.010007532661394484,\n",
       " 0.02732094404274976,\n",
       " 0.007838599858198467,\n",
       " 0.005641128073350916,\n",
       " 0.026686752521955265,\n",
       " 0.0011264816201186254,\n",
       " -0.020027748072870735,\n",
       " -0.028563957858885127,\n",
       " 0.016057713883815616,\n",
       " -0.0037892905648908857,\n",
       " 0.0038019744791258025,\n",
       " -0.022171313513258177,\n",
       " 0.0015482185670084414,\n",
       " -0.004953031139418836,\n",
       " -0.01804907328470658,\n",
       " -0.009405050809771968,\n",
       " -0.018543741516086357,\n",
       " -0.00847279044767044,\n",
       " -0.011053947087457112,\n",
       " 0.023921680173500135,\n",
       " 0.013825361742275645,\n",
       " 0.0016195649733994437,\n",
       " -0.007267827675747928,\n",
       " 0.020027748072870735,\n",
       " -0.002371874033970903,\n",
       " 0.0004114313393335181,\n",
       " 0.0005660153944703291,\n",
       " 0.012220858815166765,\n",
       " 0.02261524701902081,\n",
       " -0.0007665782603589392,\n",
       " 0.009214793726062629,\n",
       " 0.002005628658886101,\n",
       " 0.0028538590496076555,\n",
       " 0.013964884100367838,\n",
       " -0.0040271121523581905,\n",
       " 0.027016531963756796,\n",
       " 0.02553252540697242,\n",
       " -0.002663601733067686,\n",
       " -0.031075354716609487,\n",
       " -0.000566808124558097,\n",
       " -0.0038178293137117905,\n",
       " ...]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.embed_query(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ad14edc-3140-490d-b297-80fb607d24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.embed_query(\"How are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9eeb50fa-6613-448a-8056-374a06799ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '1eea3b44-8078-45af-9a31-8c18dbebe9bd')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e1e85824-a1b5-496a-9825-e29ad6754be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "88954132-8d34-41db-b122-bdf78777d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP\tus-west-1 (N. California)\tus-west1-gcp\n",
    "# GCP\tus-central-1 (Iowa)\tus-central1-gcp\n",
    "# GCP\tus-west-4 (Las Vegas)\tus-west4-gcp\n",
    "# GCP\tus-east-4 (Virginia)\tus-east4-gcp\n",
    "# GCP\tnorthamerica-northeast-1\tnorthamerica-northeast1-gcp\n",
    "# GCP\tasia-northeast-1 (Japan)\tasia-northeast1-gcp\n",
    "# GCP\tasia-southeast-1 (Singapore)\tasia-southeast1-gcp\n",
    "# GCP\tus-east-1 (South Carolina)\tus-east1-gcp\n",
    "# GCP\teu-west-1 (Belgium)\teu-west1-gcp\n",
    "# GCP\teu-west-4 (Netherlands)\teu-west4-gcp\n",
    "# AWS\tus-east-1 (Virginia)\tus-east-1-aws\n",
    "# Azure\teastus (Virginia)\teastus-azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "868241e0-0763-4623-82cc-1b1aa714d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7b9df689-d955-4475-88e0-93c2a816ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "49cc1f65-6924-494f-8c3e-d5d05c9dd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "vectorstore = pc.create_index(\n",
    "  name=index_name,\n",
    "  dimension=1536,\n",
    "  metric=\"cosine\",\n",
    "  spec=PodSpec(\n",
    "    environment=\"gcp-starter\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "613e333b-cc67-49a9-901d-7eeaf5a956c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa9f8c-a6e1-4704-8fb2-fdf953b56dbb",
   "metadata": {},
   "source": [
    "## Create Embeddings for each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1dce41-142a-4041-bef2-7d29886cbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = index.upsert([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bd4707e6-9fe9-46b4-a6e6-9cdfe4dc4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Pinecone as PineconeLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd20fac-b662-41a7-91fb-749d7ad44945",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65fc39-bf6c-40b5-8332-f608590cc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39e558-434f-41d5-b6ed-24ff863d3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a1a7a-f3da-463b-b8a5-80871a696afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ea4b6-6b5c-48d7-aeac-6e6ed79a75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbff9ae-d858-4dba-9d56-9884844adb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02238857-adde-4f78-ac7b-c263bb7c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6d421-bf94-464c-ba4a-1282c79acbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7f6ce-2d41-450b-960c-012213d19b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e9ec9-ed00-46bb-850e-da89cd5c8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "while True:\n",
    "  user_input = input(f\"Input Prompt: \")\n",
    "  if user_input == 'exit':\n",
    "    print('Exiting')\n",
    "    sys.exit()\n",
    "  if user_input == '':\n",
    "    continue\n",
    "  result = qa({'query': user_input})\n",
    "  print(f\"Answer: {result['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
